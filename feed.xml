<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://deveshparagiri.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deveshparagiri.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-07-25T00:49:01+00:00</updated><id>https://deveshparagiri.github.io/feed.xml</id><title type="html">blank</title><subtitle>Dev Paragiri&apos;s personal website. </subtitle><entry><title type="html">Deep Dive into my Reinforcement Learning Failures</title><link href="https://deveshparagiri.github.io/blog/2025/rl-fail/" rel="alternate" type="text/html" title="Deep Dive into my Reinforcement Learning Failures"/><published>2025-07-24T16:00:00+00:00</published><updated>2025-07-24T16:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/rl-fail</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/rl-fail/"><![CDATA[<div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>TL;DR: I built an AI pricing agent that consistently performed 30-40% worse
than random pricing, teaching valuable lessons about RL reward engineering
and business applications. Now this is a work in progress and I intend to
experiment different things to see what sticks, and what doesn't.
</code></pre></div></div> <hr/> <h2 id="introduction"><strong>Introduction</strong></h2> <hr/> <ul> <li><strong>Goal</strong>: Train a reinforcement learning agent to learn optimal pricing strategies for a SaaS product</li> <li><strong>Expected</strong>: AI outperforms random pricing by 15-30%</li> <li><strong>Reality</strong>: AI consistently underperformed by 30-40%</li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl_fail/final_pricing_comparison_v3.png" sizes="95vw"/> <img src="/assets/img/rl_fail/final_pricing_comparison_v3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – Final Results Comparison</figcaption> </figure> </div> </div> <hr/> <h2 id="three-iterations-of-failure"><strong>Three Iterations of Failure</strong></h2> <hr/> <p>I attempted three different approaches, each failing in its own instructive way.</p> <h5 id="version-1-price-crasher-behavior"><strong>Version 1: Price Crasher Behavior</strong></h5> <p>The agent learned to minimize prices to nearly zero, optimizing for customer acquisition volume rather than revenue maximization.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl_fail/pricing_comparison.png" sizes="95vw"/> <img src="/assets/img/rl_fail/pricing_comparison.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Version 1 Results</figcaption> </figure> </div> </div> <h5 id="version-2-penalty-avoidance-strategy"><strong>Version 2: Penalty Avoidance Strategy</strong></h5> <p>I added penalties for extreme pricing. The agent learned to avoid penalties while maintaining suboptimal low-price strategies, performing even worse.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/rl_fail/pricing_comparison_v2.png" sizes="95vw"/> <img src="/assets/img/rl_fail/pricing_comparison_v2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – Version 2 Results</figcaption> </figure> </div> </div> <h5 id="version-3-comprehensive-reward-engineering"><strong>Version 3: Comprehensive Reward Engineering</strong></h5> <p>Despite sophisticated reward shaping with multiple incentive mechanisms, the agent maintained preference for the $10-20 price range.</p> <hr/> <table> <thead> <tr> <th>Agent Version</th> <th>Revenue</th> <th>Strategy</th> <th>Performance</th> </tr> </thead> <tbody> <tr> <td>Random Policy</td> <td>$193.26</td> <td>Varied</td> <td>Baseline</td> </tr> <tr> <td>PPO v1</td> <td>$138.81</td> <td>Price Crashing</td> <td>-31.1%</td> </tr> <tr> <td>PPO v2</td> <td>$86.49</td> <td>Penalty Avoidance</td> <td>-62.7%</td> </tr> <tr> <td>PPO v3</td> <td>$123.93</td> <td>Stubborn Underpricing</td> <td>-35.9%</td> </tr> <tr> <td><strong>Theoretical Optimal</strong></td> <td><strong>$229.22</strong></td> <td><strong>Fixed at $50</strong></td> <td><strong>+18.6%</strong></td> </tr> </tbody> </table> <hr/> <h2 id="why-this-kept-happening-core-failure-modes"><strong>Why This Kept Happening: Core Failure Modes</strong></h2> <hr/> <h5 id="local-optimization-traps"><strong>Local Optimization Traps</strong></h5> <hr/> <p>The agent consistently converged to local minima, prioritizing customer acquisition metrics over revenue optimization. Despite multiple reward engineering attempts, it never escaped this fundamental misalignment.</p> <h5 id="insufficient-exploration"><strong>Insufficient Exploration</strong></h5> <hr/> <p>Even with exploration strategies, the agent failed to adequately explore higher-price regions that could yield superior long-term rewards. The immediate positive feedback from customer signups was too compelling.</p> <h5 id="temporal-credit-assignment-challenges"><strong>Temporal Credit Assignment Challenges</strong></h5> <hr/> <p>The relationship between pricing decisions and revenue outcomes involves complex, non-linear, and delayed feedback signals. The agent demonstrated consistent preference for immediate positive feedback rather than optimizing for long-term objectives.</p> <hr/> <h2 id="lessons-learned"><strong>Lessons Learned</strong></h2> <hr/> <h5 id="reward-engineering-is-extraordinarily-difficult"><strong>Reward Engineering is Extraordinarily Difficult</strong></h5> <hr/> <p>Designing effective reward functions for business applications requires extensive domain knowledge and careful consideration of unintended optimization behaviors. Even sophisticated multi-component reward systems can fail spectacularly.</p> <hr/> <h5 id="simple-baselines-can-be-surprisingly-robust"><strong>Simple Baselines Can Be Surprisingly Robust</strong></h5> <hr/> <p>Random pricing with reasonable bounds achieved 84% of theoretical optimal revenue. This highlights how human intuition and domain knowledge often provide more reliable guidance than sophisticated algorithmic optimization in well-understood problem domains.</p> <hr/> <h5 id="algorithm-selection-matters-more-than-sophistication"><strong>Algorithm Selection Matters More Than Sophistication</strong></h5> <hr/> <p>Reinforcement learning may not be optimal for business problems with clear mathematical relationships and well-established optimization techniques. Alternative approaches often provide superior reliability:</p> <ul> <li>Bayesian optimization for systematic price point exploration</li> <li>Multi-armed bandit algorithms for statistical A/B testing</li> <li>Classical mathematical optimization for analytical solutions</li> <li>Rule-based systems incorporating domain expertise</li> </ul> <hr/> <h5 id="evaluation-and-baseline-comparison-is-critical"><strong>Evaluation and Baseline Comparison is Critical</strong></h5> <hr/> <p>The consistent underperformance revealed fundamental limitations that might have been missed without proper baseline comparisons. Sometimes the most educational projects are those that fail to meet initial expectations.</p> <hr/> <h2 id="conclusion"><strong>Conclusion</strong></h2> <hr/> <p>While the reinforcement learning agent never outperformed random pricing, this investigation provided valuable insights into reward engineering complexity, baseline comparison importance, and the limitations of RL in certain business applications. I’m quite curious on how effective random pricing with reasonable constraints can be in such usecases.</p> <hr/>]]></content><author><name></name></author><category term="code"/><category term="ai"/><summary type="html"><![CDATA[TL;DR: I built an AI pricing agent that consistently performed 30-40% worse than random pricing, teaching valuable lessons about RL reward engineering and business applications. Now this is a work in progress and I intend to experiment different things to see what sticks, and what doesn't.]]></summary></entry><entry><title type="html">Fetch-Weighted Tower Analysis of Canopy Mortality at US-MPJ</title><link href="https://deveshparagiri.github.io/blog/2025/fetch/" rel="alternate" type="text/html" title="Fetch-Weighted Tower Analysis of Canopy Mortality at US-MPJ"/><published>2025-07-14T16:00:00+00:00</published><updated>2025-07-14T16:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/fetch</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/fetch/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <hr/> <ul> <li><strong>Goal</strong>: Understand vegetation change and mortality around the US-MPJ flux tower</li> <li><strong>Motivation</strong>: Prior studies used a simple radial extraction (e.g. 200 m radius) to link predicted tree class maps to tower fluxes like GPP</li> <li><strong>Limitation</strong>: This radial assumption ignores <strong>atmospheric fetch directionality and influence distribution</strong></li> </ul> <hr/> <h2 id="limitations-of-simple-radial-analysis"><strong>Limitations of Simple Radial Analysis</strong></h2> <hr/> <ul> <li>The original 200 m circular masks assume isotropic contribution from surrounding pixels</li> <li>GPP (Gross Primary Productivity) measured by eddy covariance towers is influenced by a <strong>non-uniform footprint</strong></li> <li>This led us to revisit the approach using <strong>physical footprint modeling</strong></li> </ul> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/predictions.png" sizes="95vw"/> <img src="/assets/img/model_eval/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – 200m Radial Class Trends</figcaption> </figure> </div> </div> <hr/> <h2 id="fetch-weighted-modeling-approach"><strong>Fetch-Weighted Modeling Approach</strong></h2> <hr/> <p>Our approach utilizes half-hourly Ameriflux tower meteorology data, including wind speed (WS), wind direction (WD), friction velocity (USTAR), measurement height (zm), roughness length (z0), and canopy height (h).</p> <p>For each year of analysis, we first extract all valid half-hourly meteorological records from the tower data. For each individual record, we then compute a 2D footprint using a custom Gaussian spread kernel that accounts for atmospheric dispersion patterns.</p> <p>Finally, we accumulate and normalize these individual footprints across all valid records to produce an annual influence map <code class="language-plaintext highlighter-rouge">W</code> that represents the spatial distribution of tower measurement sensitivity.</p> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/plot.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Footprint Weight Density</figcaption> </figure> </div> </div> <hr/> <h2 id="fetch-weighted-class-composition-trends"><strong>Fetch-Weighted Class Composition Trends</strong></h2> <hr/> <p>For each year, we projected the annual footprint weight map <code class="language-plaintext highlighter-rouge">W</code> onto the corresponding prediction raster (classes: LIVE = 0, DEAD = 1, BARE = 2). Each <code class="language-plaintext highlighter-rouge">W</code> was interpolated to match the raster resolution.</p> <p>We then computed fetch-weighted class fractions using:</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>LIVE_weighted <span class="o">=</span> ∑ W[i,j] × <span class="o">(</span>P[i,j] <span class="o">==</span> 0<span class="o">)</span>
DEAD_weighted <span class="o">=</span> ∑ W[i,j] × <span class="o">(</span>P[i,j] <span class="o">==</span> 1<span class="o">)</span>
BARE_weighted <span class="o">=</span> ∑ W[i,j] × <span class="o">(</span>P[i,j] <span class="o">==</span> 2<span class="o">)</span>
</code></pre></div></div> <p>Since <code class="language-plaintext highlighter-rouge">W</code> is normalized (Σ W = 1), these directly represent the percent contribution of each class to the tower’s GPP footprint.</p> <p>Confidence intervals were derived using model precision:</p> <ul> <li>LIVE: 95%</li> <li>DEAD: 62%</li> <li>BARE: 87%</li> </ul> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/predictions.png" sizes="95vw"/> <img src="/assets/img/model_eval/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/fetch_plot.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/fetch_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The fetch-weighted analysis reveals a more pronounced decline in DEAD class composition and demonstrates greater stability in BARE class trends compared to the simple radial approach.</p> <hr/> <h2 id="understanding-activation-weight-distribution"><strong>Understanding Activation: Weight Distribution</strong></h2> <hr/> <p>The distribution of footprint weights <code class="language-plaintext highlighter-rouge">W</code> is extremely skewed. While the full raster may span up to 1 km², only a small fraction of pixels meaningfully influence the tower signal. Most pixels have weights near zero.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/distr.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/distr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This confirms that fetch-based analysis is fundamentally different from circular masking. A 200 m radius includes thousands of irrelevant pixels, whereas fetch weighting highlights only the core contributing region.</p> <hr/> <h3 id="visualizing-activation-regions">Visualizing Activation Regions</h3> <ul> <li>Overlaid high-weight contours on prediction maps to reveal where influence was concentrated</li> <li>Only a small fraction of pixels within the 200 m zone carried meaningful weight</li> </ul> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/contour.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/contour.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h2 id="next-steps"><strong>Next Steps</strong></h2> <hr/> <ul> <li>Expand analysis to a <strong>1 km² area</strong></li> <li>Integrate <strong>higher-frequency (HH) Ameriflux data</strong> to refine temporal precision</li> <li>Combine annual footprints with <strong>seasonal GPP partitions</strong></li> <li>Use <strong>cumulative multi-year fetch</strong> to study legacy effects</li> <li>Run <strong>per-class NDVI or canopy height</strong> trends within weighted zone</li> </ul> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Transition Analysis and Validation with AmeriFlux GPP</title><link href="https://deveshparagiri.github.io/blog/2025/model-eval/" rel="alternate" type="text/html" title="Transition Analysis and Validation with AmeriFlux GPP"/><published>2025-06-24T16:00:00+00:00</published><updated>2025-06-24T16:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/model-eval</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/model-eval/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <hr/> <p>The US-MPJ site has exhibited conflicting trends in ecosystem productivity from 2010 onward. While ED-LiDAR reconstructions indicate stable or rising productivity, both Landsat NDVI and AmeriFlux tower observations (GPP) show a marked decline. This divergence raises a key question: Is rapid, large-scale canopy mortality being missed by traditional models?</p> <p>To investigate, we developed a lightweight image-based classifier using high-resolution NAIP aerial imagery to directly detect tree mortality. We then analyzed class transitions across time (2014–2022) and validated findings against tower-based GPP observations.</p> <hr/> <h2 id="model-retraining-with-updated-labels"><strong>Model Retraining with Updated Labels</strong></h2> <hr/> <p>We curated a refined labeled dataset of 1500 high-confidence samples across three classes: <code class="language-plaintext highlighter-rouge">LIVE, DEAD, BARE</code>. Labeling combined NDVI-based filtering and visual inspection across 5 NAIP years.</p> <p>For each pixel, we extracted:</p> <ul> <li>Red, Green, Blue, NIR</li> <li>NDVI = (NIR - Red) / (NIR + Red)</li> </ul> <hr/> <h5 id="classifier-details"><strong>Classifier Details</strong></h5> <ul> <li><strong>Model</strong>: Random Forest (100 trees)</li> <li><strong>Sampling</strong>: Stratified 80/20 train-test split</li> <li><strong>Balancing</strong>: Class weights set to “balanced”</li> <li><strong>Accuracy</strong>: 0.838</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support
       LIVE       0.86      0.81      0.83        31
       DEAD       0.75      0.78      0.77        23
       BARE       0.90      0.95      0.93        20
</code></pre></div></div> <hr/> <h2 id="temporal-transition-analysis-20142022"><strong>Temporal Transition Analysis (2014–2022)</strong></h2> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/predictions.png" sizes="95vw"/> <img src="/assets/img/model_eval/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – Heatmap of Plausibility (Simple Score) Across Fixed Drift Combinations</figcaption> </figure> </div> </div> <p>For each pair of years, we exhaustively tested 625 spatial drift combinations, and selected the configuration minimizing biologically implausible transitions (e.g., DEAD → BARE)</p> <hr/> <table> <thead> <tr> <th>Year Pair</th> <th>Best Drift</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td>2014 → 2016</td> <td>(0, 3, 0, 4)</td> <td>Minor LIVE → DEAD, DEAD → BARE</td> </tr> <tr> <td>2016 → 2018</td> <td>(0, 3, 0, 2)</td> <td>Significant DEAD → BARE</td> </tr> <tr> <td>2018 → 2020</td> <td>(0, 0, 0, 1)</td> <td>LIVE → LIVE recovery pattern</td> </tr> <tr> <td>2020 → 2022</td> <td>(0, 1, 0, 0)</td> <td>BARE plateaued</td> </tr> </tbody> </table> <p><br/></p> <h4 id="observations"><strong>Observations</strong></h4> <ul> <li>The 2016–2018 period shows the most pronounced shift toward BARE</li> <li>Post-2020 suggests stabilization</li> </ul> <hr/> <h2 id="pixel-class-trends-over-time"><strong>Pixel Class Trends Over Time</strong></h2> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/r.png" sizes="95vw"/> <img src="/assets/img/model_eval/r.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Raster Trend</figcaption> </figure> </div> </div> <p><strong>Confidence Intervals:</strong> Based on classifier precision (LIVE: 95%, DEAD: 62%, BARE: 87%)</p> <ul> <li><strong>BARE</strong>: Steady increase throughout</li> <li><strong>DEAD</strong>: Falls after 2018, partial rebound by 2022</li> <li><strong>LIVE</strong>: Rises until 2020, then declines sharply</li> </ul> <hr/> <h2 id="cross-validation-with-ameriflux-gpp"><strong>Cross-Validation with AmeriFlux GPP</strong></h2> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/gpp.png" sizes="95vw"/> <img src="/assets/img/model_eval/gpp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – GPP vs Model</figcaption> </figure> </div> </div> <p>This analysis is based on AmeriFlux GPP data from the US-MPJ site.</p> <ul> <li>Spearman ρ = <strong>-0.800</strong>, p = 0.200 (n = 4)</li> <li>Moderate-to-strong negative trend between canopy loss and productivity</li> </ul> <p>Despite small sample size, the directionality supports hypothesis. BARE% rise coincides with sharp GPP fall (2016–2020)</p> <hr/> <h2 id="discussion"><strong>Discussion</strong></h2> <hr/> <ul> <li><strong>Drift correction</strong> improves temporal consistency in pixel-wise transitions</li> <li><strong>Model confidence</strong> (especially for BARE) lends weight to ecological interpretation</li> <li><strong>ED-LiDAR reconstructions</strong> likely miss rapid disturbance pulses</li> </ul> <p><strong>Limitations:</strong> Spatial resolution mismatch between tower and NAIP raster, small test set; more ground truth would improve model robustness</p> <hr/> <h2 id="conclusion"><strong>Conclusion</strong></h2> <hr/> <p>Our approach demonstrates that lightweight, image-based classifiers can reveal large-scale canopy mortality trends consistent with independent tower and satellite records. These models offer a promising supplement to traditional ecological reconstructions.</p> <p>Next steps:</p> <ul> <li>Apply SIFT for more complex transition matching</li> <li>Validate with 2022 AmeriFlux GPP</li> <li>Package the tool for broader deployment in mortality monitoring</li> </ul> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Drift Configuration Evaluation and Transition Matrix Optimization</title><link href="https://deveshparagiri.github.io/blog/2025/drift-configuration-eval-copy/" rel="alternate" type="text/html" title="Drift Configuration Evaluation and Transition Matrix Optimization"/><published>2025-06-14T16:00:00+00:00</published><updated>2025-06-14T16:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/drift-configuration-eval%20copy</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/drift-configuration-eval-copy/"><![CDATA[<h2 id="overview"><strong>Overview</strong></h2> <p>Building on our previous work analyzing spatial drift in NAIP imagery, we now shift focus to discovering <strong>globally optimal drift configurations</strong>. Specifically, we aim to:</p> <ul> <li>Evaluate all <strong>625 possible drift configurations</strong> (0–4 pixel shifts in row and column) between two years.</li> <li>Score each configuration using biologically inspired heuristics.</li> <li>Visualize and compare <strong>transition matrices</strong> from fixed vs. drift-corrected rasters.</li> <li>Statistically validate improvements in <strong>NDVI temporal consistency</strong> and <strong>transition plausibility</strong>.</li> </ul> <p>This report also introduces two scoring mechanisms to distill each 3×3 transition matrix into a single numeric value:</p> <ul> <li><strong>Simple Score</strong>: A linear weighting that rewards identity transitions and penalizes implausible ones.</li> <li><strong>Composite Score</strong>: A multi-factored function accounting for ecological decay patterns, entropy (stability), and biologically implausible reversals.</li> </ul> <hr/> <p><br/></p> <h2 id="why-625-drift-configurations"><strong>Why 625 Drift Configurations?</strong></h2> <p>NAIP imagery has a <strong>maximum documented spatial error of ±4 meters</strong>, or <strong>4 pixels (1m resolution)</strong> in either direction. For each pixel in <code class="language-plaintext highlighter-rouge">year 1</code>, we test all combinations of potential displacements for both the reference (<code class="language-plaintext highlighter-rouge">year 1</code>) and the target (<code class="language-plaintext highlighter-rouge">year 2</code>) images:</p> <blockquote> <p>Total drift combinations = 5 (row1) × 5 (col1) × 5 (row2) × 5 (col2) = 625</p> </blockquote> <p>For each configuration, we:</p> <ul> <li>Shift the raster accordingly.</li> <li>Compute a full <strong>3×3 transition matrix</strong>.</li> <li>Apply the scoring functions described below.</li> </ul> <hr/> <p><br/></p> <h2 id="scoring-transition-matrices"><strong>Scoring Transition Matrices</strong></h2> <p><br/></p> <h3 id="simple-score"><strong>Simple Score</strong></h3> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SCORE = +1 × identity transitions (LIVE→LIVE, DEAD→DEAD, BARE→BARE)
       - 3 × implausible transitions (e.g., BARE→LIVE, DEAD→LIVE, BARE→DEAD)
       - 2 × LIVE→BARE, DEAD→BARE
</code></pre></div></div> <p>This score linearly rewards stability and penalizes biologically suspect changes. Fast to compute, it provides an interpretable surface over the 625 space.</p> <blockquote> <p>We visualized this in 2D heatmaps (fixed dr1, dc1) and a 3D surface showing plausibility as a function of relative drift ∆r, ∆c.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/reldrift2d.png" sizes="95vw"/> <img src="/assets/img/drift_config/reldrift2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – Heatmap of Plausibility (Simple Score) Across Fixed Drift Combinations</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/reldrift3d.png" sizes="95vw"/> <img src="/assets/img/drift_config/reldrift3d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – 3D Surface Plot of Drift ∆r, ∆c vs Score</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="composite-score"><strong>Composite Score</strong></h3> <p>The composite score integrates:</p> <ul> <li><strong>Decay Reward</strong>: Encourages LIVE → DEAD → BARE progression</li> <li><strong>Implausibility Penalty</strong>: Penalizes unnatural reversals (e.g., BARE → DEAD)</li> <li><strong>Entropy Penalty</strong>: Penalizes instability in row-wise transition distributions</li> </ul> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SCORE = -Entropy + Decay Reward - Implausible Penalty
</code></pre></div></div> <p>This metric encodes more biological realism and rewards transitions that align with ecological degradation.</p> <p>We plotted this across all 625 configurations and observed a <strong>long-tailed unimodal curve</strong>:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/composite_score_plot.png" sizes="95vw"/> <img src="/assets/img/drift_config/composite_score_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – Composite Score Distribution Over Drift Space</figcaption> </figure> </div> </div> <hr/> <h2 id="comparing-best-drift-configs"><strong>Comparing Best Drift Configs</strong></h2> <p>We selected the top-scoring drift configurations under both schemes:</p> <ul> <li><strong>Simple Score</strong>: <code class="language-plaintext highlighter-rouge">(0, 3, 0, 4)</code></li> <li><strong>Composite Score</strong>: <code class="language-plaintext highlighter-rouge">(1, 3, 1, 1)</code></li> </ul> <p>Below are the resulting transition matrices compared to the baseline (no drift):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/simple_compare.png" sizes="95vw"/> <img src="/assets/img/drift_config/simple_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4 – Simple Drift vs No Drift (Transition Matrix %)</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/composite_compare.png" sizes="95vw"/> <img src="/assets/img/drift_config/composite_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5 – Composite Drift vs No Drift (Transition Matrix %)</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/simple_composite_compare.png" sizes="95vw"/> <img src="/assets/img/drift_config/simple_composite_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6 – Simple vs Composite Drift (Transition Matrix %)</figcaption> </figure> </div> </div> <p><br/></p> <h4 id="observation"><strong>Observation</strong></h4> <ul> <li>Simple drift yielded cleaner BARE → BARE diagonals and minimized reversal transitions</li> <li>Composite score slightly favored plausible but diverse transitions with less focus on strict diagonal preservation</li> </ul> <hr/> <p><br/></p> <h4 id="note-on-edge-effects-and-pixel-exclusion"><strong>Note on Edge Effects and Pixel Exclusion</strong></h4> <ul> <li> <p>When applying spatial drift configurations (e.g., shifting pixels up to 4 rows/columns in each direction), some border regions in the raster fall outside the valid image bounds. To ensure consistency and prevent out-of-bounds indexing errors, these edge pixels were excluded from all transition and NDVI calculations.</p> </li> <li> <p>As a result, the total number of pixels used in each drift configuration may vary slightly depending on how much drift was applied, especially in diagonally extreme configurations. These excluded pixels do not affect the comparative plausibility scoring, as the variation is negligible (&lt;3% of total pixels), and only valid overlapping regions were considered in all evaluations.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/count_distribution.png" sizes="95vw"/> <img src="/assets/img/drift_config/count_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="validating-ndvi-temporal-stability"><strong>Validating NDVI Temporal Stability</strong></h2> <p>To evaluate temporal alignment, we computed <strong>pixelwise NDVI standard deviation</strong> across 5 years (2014–2022) for all interior pixels:</p> <ul> <li><strong>Fixed Coordinates</strong>: Use same pixel (r, c) each year</li> <li><strong>Drift-Corrected</strong>: Apply optimal drift (from 2014 and 2016 only) then use fixed (r, c) for future years</li> </ul> <p><strong>Result:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Fixed</th> <th>Drift-Corrected</th> </tr> </thead> <tbody> <tr> <td>Mean NDVI Std Deviation</td> <td>0.04544</td> <td>0.05093</td> </tr> <tr> <td>Paired t-test p-value</td> <td><strong>&lt; 0.000001</strong></td> <td>(statistically significant)</td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/ndvi.png" sizes="95vw"/> <img src="/assets/img/drift_config/ndvi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While the mean NDVI variability increased slightly under drift correction, the test remains a useful lens for validating long-term alignment.</p> <hr/> <p><br/></p> <h2 id="observing-score-differences"><strong>Observing Score Differences</strong></h2> <p>We analyzed the <strong>Spearman Rank Correlation</strong> between both scoring systems:</p> <blockquote> <p>ρ = -0.2253, p &lt; 0.000001 → Significant negative correlation</p> </blockquote> <p>This suggests the scoring systems are <strong>not directly aligned</strong>, and each captures different facets of temporal plausibility</p> <hr/> <p><br/></p> <h2 id="final-takeaways"><strong>Final Takeaways</strong></h2> <ul> <li>Optimal drift varies depending on scoring design and ecological intent</li> <li>Simple scoring performs surprisingly well for suppressing implausible transitions</li> <li>Composite scoring emphasizes ecological progression and entropy</li> <li>NDVI-based variance is slightly worse under drift correction, but this may reflect better alignment with shifting canopies or shadows</li> <li>This pipeline generalizes across time ranges and can be applied to all pairwise year transitions</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Apply the same analysis for 2016 → 2018, 2018 → 2020, 2020 → 2022</li> <li>Test alternate scoring functions (e.g., chi-squared divergence from expected decay)</li> <li>Integrate the best drift config into training/validation for improved pixel classification</li> <li>Track class-specific drift stability (e.g., BARE pixels across years)</li> <li>Eventually aim for full spatiotemporal drift-corrected land cover analysis</li> </ul> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Analyzing Spatial Drift in Aerial Imagery - Implications for Temporal Pixel Classification</title><link href="https://deveshparagiri.github.io/blog/2025/analyzing-spatial-drift/" rel="alternate" type="text/html" title="Analyzing Spatial Drift in Aerial Imagery - Implications for Temporal Pixel Classification"/><published>2025-06-04T16:00:00+00:00</published><updated>2025-06-04T16:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/analyzing-spatial-drift</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/analyzing-spatial-drift/"><![CDATA[<h2 id="motivation"><strong>Motivation</strong></h2> <p>Accurate analysis of temporal vegetation transitions—such as LIVE → DEAD or DEAD → BARE—requires reliable alignment of pixels across years. Even after spatial preprocessing like warping and histogram matching, small-scale spatial drift can occur, especially in high-resolution NAIP imagery.</p> <p>This poses a significant problem for longitudinal classification. If the same (row, col) location in one year corresponds to a slightly shifted feature in another (e.g., shadow, soil), observed changes may reflect misalignment rather than ecological dynamics.</p> <p>To rigorously evaluate class transitions and develop robust temporal models, we must quantify and control for this drift.</p> <hr/> <p><br/></p> <h2 id="identifying-a-fixed-reference-point"><strong>Identifying a Fixed Reference Point</strong></h2> <p>To ground the analysis, we manually located a visually bright, highly consistent white patch—most likely a man-made structure or tower—within the 2014 NAIP image. This was done using RGB previews of the matched<em>buffer</em>{year}.tif files in QGIS.</p> <p>We then traced this patch across earlier years and visually validated its shifted position in 2016 and 2018. The corresponding row/column locations are:</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>Row</strong></th> <th><strong>Col</strong></th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>119</td> <td>224</td> </tr> <tr> <td>2016</td> <td>121</td> <td>224</td> </tr> <tr> <td>2018</td> <td>118</td> <td>224</td> </tr> </tbody> </table> <p><br/> No precise match could be confirmed for 2020 and 2022, likely due to changes in lighting conditions or NDVI spectral compression in post-processing.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/fixedpixels.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/fixedpixels.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – 2x2 Patch Across Years (Visual RGB Validation)</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="understanding-spatial-drift"><strong>Understanding Spatial Drift</strong></h2> <p>We define three conceptual degrees of spatial drift:</p> <ul> <li> <h6 id="level-1-linear-shift"><strong>Level 1: Linear Shift</strong></h6> <p>Straightforward pixel-level movement (±1–3 pixels), often due to image resampling or slight registration error.</p> </li> <li> <h6 id="level-2-rotationalangular-misalignment"><strong>Level 2: Rotational/Angular Misalignment</strong></h6> <p>Small-angle shifts or skewing that change the neighborhood context of a patch (i.e., rotated trees or canopy boundaries).</p> </li> <li> <h6 id="level-3-raster-wide-nonlinear-drift"><strong>Level 3: Raster-Wide Nonlinear Drift</strong></h6> <p>Region-specific distortions or warping effects that cannot be corrected via uniform translation.</p> </li> </ul> <p>In this study, we focused on evaluating Level 1 drift at the patch (5×5) and pixel levels.</p> <hr/> <h2 id="experimental-design"><strong>Experimental Design</strong></h2> <p>Given the difficulty of locating more visually stable points, we designed a randomized experiment to test whether NDVI time-series stability improves when correcting for drift.</p> <h3 id="key-questions"><strong>Key Questions</strong></h3> <ul> <li>Does accounting for local drift improve NDVI consistency over time?</li> <li>How does this effect vary between single pixels and aggregated 5×5 patches?</li> </ul> <h3 id="method"><strong>Method</strong></h3> <p>We sampled <strong>50 locations</strong> from the buffer region:</p> <ul> <li><strong>3 manually verified drifted tower locations</strong></li> <li><strong>47 random locations</strong> from valid areas of the 2014 image</li> </ul> <p>For each location, we:</p> <ul> <li>Extracted NDVI time-series from the same fixed coordinate over five years</li> <li>Applied drift correction by finding the best-matching 5×5 patch (or pixel) using RGB MSE</li> <li>Calculated standard deviation of NDVI across time in both cases</li> <li>Performed a paired t-test comparing NDVI temporal variability (std deviation) before and after correction</li> </ul> <hr/> <p><br/></p> <h2 id="results"><strong>Results</strong></h2> <p><br/></p> <h3 id="patch-level-55"><strong>Patch-Level (5×5)</strong></h3> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Fixed</strong></th> <th><strong>Corrected</strong></th> </tr> </thead> <tbody> <tr> <td>Mean NDVI std</td> <td>0.01902</td> <td>0.02078</td> </tr> <tr> <td>T-statistic</td> <td>-1.5629</td> <td> </td> </tr> <tr> <td>P-value</td> <td>0.12452</td> <td> </td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/patchdrift.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/patchdrift.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Histogram: NDVI Std Dev (5×5 Patch-Level, Fixed vs Corrected)</figcaption> </figure> </div> </div> <p><strong>Interpretation:</strong></p> <p>NDVI temporal variance was slightly higher after drift correction, but the difference was not statistically significant. This suggests that 5×5 patches may already average out small shifts, providing inherent spatial robustness.</p> <hr/> <p><br/></p> <h3 id="pixel-level-11"><strong>Pixel-Level (1×1)</strong></h3> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Fixed</strong></th> <th><strong>Corrected</strong></th> </tr> </thead> <tbody> <tr> <td>Mean NDVI std</td> <td>0.03982</td> <td>0.02163</td> </tr> <tr> <td>T-statistic</td> <td>5.4654</td> <td> </td> </tr> <tr> <td>P-value</td> <td>0.000002</td> <td> </td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/pixeldrift.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/pixeldrift.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – Histogram: NDVI Std Dev (Pixel-Level, Fixed vs Corrected)</figcaption> </figure> </div> </div> <p><strong>Interpretation:</strong></p> <p>At the pixel level, drift correction substantially reduced NDVI variance over time. This indicates that raw pixel comparisons are highly sensitive to even minor misalignments, validating the need for patch-based or drift-corrected strategies in pixelwise classification.</p> <hr/> <p><br/></p> <h2 id="implications-for-temporal-classification"><strong>Implications for Temporal Classification</strong></h2> <p>This experiment confirms that spatial drift—though subtle—can meaningfully distort pixel-level change analysis. Without drift correction:</p> <ul> <li>Apparent transitions may be artifacts</li> <li>NDVI profiles become unstable</li> <li>Model errors can accumulate over time</li> </ul> <p>In contrast, aggregating over 5×5 patches appears to mitigate these issues. For temporal studies involving land cover change, forest degradation, or regrowth detection, we recommend either:</p> <ol> <li>Drift correction per-pixel when doing fine-grained transition modeling, or</li> <li>Switching to patch-based classification frameworks.</li> </ol> <hr/> <p><br/></p> <h2 id="limitations-and-next-steps"><strong>Limitations and Next Steps</strong></h2> <ul> <li>Only 3 fixed reference points could be visually confirmed across years</li> <li>Drift was only modeled as linear (search radius ±3 pixels)</li> <li>NDVI was the only signal used for comparison (future versions may include full spectral MSE)</li> </ul> <hr/> <p><br/></p> <h2 id="class-transition-validation-under-drift-correction"><strong>Class Transition Validation Under Drift Correction</strong></h2> <p>To further quantify the impact of spatial drift, we evaluated how <strong>class transitions across years</strong> are affected by fixed vs. drift-corrected sampling. The goal was to check whether implausible transitions—e.g., BARE → DEAD or DEAD → LIVE—are more common when drift is not accounted for.</p> <hr/> <p><br/></p> <h3 id="motivation-1"><strong>Motivation</strong></h3> <p>Temporal classification relies not just on stable NDVI signals but also on <strong>reasonable class transitions</strong>. Certain transitions are biologically plausible (e.g., LIVE → DEAD → BARE), while others are not expected without long timescales or special conditions (e.g., BARE → LIVE over 2 years).</p> <p>Spatial drift can artificially introduce these implausible sequences. This section evaluates whether correcting for drift reduces the frequency of such transitions across the entire buffer.</p> <hr/> <p><br/></p> <h3 id="hypothesis"><strong>Hypothesis</strong></h3> <p><strong>If drift correction improves temporal alignment, then:</strong></p> <ul> <li>The proportion of implausible transitions should decrease</li> <li>Heatmaps of transition probabilities should show more stability or logical progression</li> <li>This effect should be statistically significant</li> </ul> <hr/> <p><br/></p> <h3 id="method-1"><strong>Method</strong></h3> <p>For each pixel in the valid interior region of the buffer:</p> <ol> <li><strong>Extract Class Predictions</strong> from 2014, 2016, and 2018</li> <li><strong>Construct Fixed Sequences</strong>: row/col is held constant</li> <li><strong>Construct Drift-Corrected Sequences</strong>: find the closest RGB patch match in the next year using 5×5 RGB context</li> <li><strong>Count transitions</strong> of form: (class_t → class_t+1) for each consecutive year pair</li> <li><strong>Tabulate</strong> transition matrices for both fixed and corrected cases</li> <li><strong>Highlight implausible transitions</strong>, defined as: <ul> <li>DEAD → LIVE</li> <li>BARE → DEAD</li> <li>BARE → LIVE</li> </ul> </li> <li><strong>Plot</strong> <ul> <li>Heatmaps of transition probabilities</li> <li>Bar plots of total implausible transition percentages</li> </ul> </li> </ol> <hr/> <p><br/></p> <h3 id="results-1"><strong>Results</strong></h3> <p><br/></p> <h3 id="heatmap-2014--2016-transitions"><strong>Heatmap: 2014 → 2016 Transitions</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/transition1.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/transition1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Heatmap: 2014 → 2016 Transition</figcaption> </figure> </div> </div> <h3 id="heatmap-2016--2018-transitions"><strong>Heatmap: 2016 → 2018 Transitions</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/transition2.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/transition2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Heatmap: 2016 → 2018 Transition</figcaption> </figure> </div> </div> <hr/> <h3 id="bar-chart-implausible-transition-rates"><strong>Bar Chart: Implausible Transition Rates</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/graph1.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/graph1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Bar Chart: Implausible Transition Rates</figcaption> </figure> </div> </div> <table> <thead> <tr> <th><strong>Transition Period</strong></th> <th><strong>Fixed (%)</strong></th> <th><strong>Corrected (%)</strong></th> </tr> </thead> <tbody> <tr> <td>2014 → 2016</td> <td>15.5%</td> <td>13.4%</td> </tr> <tr> <td>2016 → 2018</td> <td>19.8%</td> <td>14.4%</td> </tr> </tbody> </table> <p><br/> <strong>Statistical Test (Chi²):</strong></p> <ul> <li><strong>χ² Statistic:</strong> 144.07</li> <li><strong>P-value:</strong> &lt; 0.000001</li> </ul> <p>→ <strong>Statistically significant improvement</strong> in plausibility after drift correction</p> <hr/> <p><br/></p> <h3 id="interpretation"><strong>Interpretation</strong></h3> <ul> <li>Heatmaps show that <strong>drift correction increases class stability</strong>, particularly for BARE and DEAD categories.</li> <li>The <strong>frequency of implausible transitions drops significantly</strong> when drift is corrected.</li> <li>Statistical testing confirms that this reduction is unlikely to be due to chance.</li> </ul> <hr/> <p><br/></p> <h2 id="final-takeaways"><strong>Final Takeaways</strong></h2> <ul> <li>Pixel-level NDVI analysis is highly sensitive to spatial drift</li> <li>Drift correction reduces implausible transitions and enhances temporal stability</li> <li>Patch-level aggregation is somewhat resilient to drift but less interpretable</li> <li>This two-pronged analysis—NDVI time-series + transition plausibility—offers a comprehensive method for diagnosing and mitigating drift in remote sensing studies</li> </ul> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Motivation]]></summary></entry><entry><title type="html">Refining Tree Pixel Classification and Temporal Analysis with Updated Labels and Balancing</title><link href="https://deveshparagiri.github.io/blog/2025/refining-tree-pixel-classification.md/" rel="alternate" type="text/html" title="Refining Tree Pixel Classification and Temporal Analysis with Updated Labels and Balancing"/><published>2025-05-28T16:00:00+00:00</published><updated>2025-05-28T16:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/refining-tree-pixel-classification.md</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/refining-tree-pixel-classification.md/"><![CDATA[<h2 id="overview"><strong>Overview</strong></h2> <p>This report extends my previous work on classifying LIVE, DEAD, and BARE tree pixels in aerial imagery. It focuses on refining model performance through targeted labeling improvements, class rebalancing, and updated predictions across five years of NAIP imagery.</p> <p><br/></p> <h3 id="objectives"><strong>Objectives</strong></h3> <ul> <li>Improve class performance (especially DEAD) via more diverse and confident labeling</li> <li>Balance class counts to prevent overfitting to LIVE</li> <li>Apply the updated model to 2014–2022 rasters and evaluate class distribution trends</li> <li>Quantitatively validate whether observed changes in class proportions over time are statistically significant</li> <li>Explore visual and quantitative summaries of tree pixel transitions</li> </ul> <hr/> <p><br/></p> <h2 id="label-improvements-and-class-balance"><strong>Label Improvements and Class Balance</strong></h2> <p>A significant portion of this work involved adding high-confidence LIVE samples in underrepresented years (especially 2018 and 2022) and increasing the BARE count.</p> <p><br/></p> <h3 id="final-label-distribution"><strong>Final Label Distribution</strong></h3> <p><br/></p> <table> <thead> <tr> <th><strong>Class</strong></th> <th><strong>Count</strong></th> </tr> </thead> <tbody> <tr> <td>LIVE</td> <td>153</td> </tr> <tr> <td>DEAD</td> <td>113</td> </tr> <tr> <td>BARE</td> <td>103</td> </tr> </tbody> </table> <hr/> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>LIVE</strong></th> <th><strong>DEAD</strong></th> <th><strong>BARE</strong></th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>28</td> <td>23</td> <td>19</td> </tr> <tr> <td>2016</td> <td>39</td> <td>18</td> <td>16</td> </tr> <tr> <td>2018</td> <td>24</td> <td>22</td> <td>31</td> </tr> <tr> <td>2020</td> <td>38</td> <td>24</td> <td>11</td> </tr> <tr> <td>2022</td> <td>24</td> <td>26</td> <td>26</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h2 id="model-evaluation-8020-split"><strong>Model Evaluation (80/20 Split)</strong></h2> <p>Using the updated labeled dataset of 369 labeled pixels across five years, I trained a Random Forest classifier using an 80/20 stratified split and <code class="language-plaintext highlighter-rouge">class_weight="balanced"</code> to account for class imbalance. The confusion matrix and metrics below reflect performance on the held-out 20% test set.</p> <p><br/></p> <h3 id="confusion-matrix"><strong>Confusion Matrix</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/confusionmatrix.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/confusionmatrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="classification-report"><strong>Classification Report</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score
LIVE            0.95       0.67     0.78
DEAD            0.62       0.86     0.72
BARE            0.87       0.87     0.87
</code></pre></div></div> <p><br/></p> <h3 id="observations"><strong>Observations</strong></h3> <ul> <li>DEAD recall improved significantly due to class weighting and additional examples</li> <li>LIVE still dominates in precision, but shows some confusion with DEAD</li> <li>BARE remains stable and accurately learned</li> </ul> <hr/> <p><br/></p> <h2 id="raster-map-comparison-pre-vs-post"><strong>Raster Map Comparison (Pre vs Post)</strong></h2> <p>To assess model improvements in the wild, I compared raster predictions for 2020 before and after balancing:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/2020premap.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/2020premap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">2020 Prediction Map (Pre-Balanced)</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/2020postmap.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/2020postmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">2020 Prediction Map (Post-Balanced)</figcaption> </figure> </div> </div> <p><br/></p> <table> <thead> <tr> <th><strong>Class</strong></th> <th><strong>Pre-Balanced</strong></th> <th><strong>Post-Balanced</strong></th> <th><strong>Change (%)</strong></th> </tr> </thead> <tbody> <tr> <td>LIVE</td> <td>21,628</td> <td>21,241</td> <td>-1.8%</td> </tr> <tr> <td>DEAD</td> <td>30,339</td> <td>31,112</td> <td>+2.5%</td> </tr> <tr> <td>BARE</td> <td>74,156</td> <td>73,770</td> <td>-0.5%</td> </tr> </tbody> </table> <p><br/></p> <h3 id="observation"><strong>Observation</strong></h3> <p>Balancing helped reduce DEAD under prediction and slightly corrected LIVE overconfidence.</p> <hr/> <p><br/></p> <h2 id="ndvi-histogram-by-class"><strong>NDVI Histogram (By Class)</strong></h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/ndvihistogram.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/ndvihistogram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">NDVI Histogram (By Class)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="interpretation"><strong>Interpretation</strong></h3> <ul> <li>LIVE pixels form a clear NDVI peak between 0.22–0.30</li> <li>DEAD overlaps with both LIVE and BARE, consistent with its ambiguous spectral signature</li> <li>BARE clusters near NDVI &lt; 0.1</li> </ul> <hr/> <p><br/></p> <h2 id="predicted-maps-across-time-20142022"><strong>Predicted Maps Across Time (2014–2022)</strong></h2> <p>The updated model was applied to all years in the dataset.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/combined_visualization.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/combined_visualization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Predicted Rasters (All Years)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="notable-observations"><strong>Notable Observations</strong></h3> <ul> <li>2014/2016: More DEAD patches, especially in the southern region</li> <li>2018/2020: Recovery in LIVE pixels in central/northern zones</li> <li>2022: Strong LIVE presence with balanced BARE–DEAD structure</li> </ul> <hr/> <p><br/></p> <h2 id="class-distribution-trends-pixel-level"><strong>Class Distribution Trends (Pixel-Level)</strong></h2> <p><br/></p> <h3 id="grouped-bar-chart"><strong>Grouped Bar Chart</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/stackedgraph.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/stackedgraph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Pixel Distribution Bar Chart (By Class)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="line-chart-per-class-over-time"><strong>Line Chart (Per Class Over Time)</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/linegraph.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/linegraph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Mortality Trend (2014-2022)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="insights"><strong>Insights</strong></h3> <ul> <li>BARE remains consistent (~45–47%) — acts as a stable control</li> <li>DEAD rises in 2016–2020, then drops in 2022</li> <li>LIVE shows a recovery in 2022 after dip in 2016–2018</li> </ul> <hr/> <p><br/></p> <h2 id="statistical-validation-chi-square-test"><strong>Statistical Validation: Chi-Square Test</strong></h2> <p>To test whether the distribution of class predictions changed meaningfully across years, I ran a chi-square test on predicted class counts.</p> <p><br/></p> <h3 id="results"><strong>Results</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chi2 Statistic: 3688.71
Degrees of Freedom: 8
P-value: &lt; 0.000001
</code></pre></div></div> <p><br/></p> <h3 id="interpretation-1"><strong>Interpretation</strong></h3> <p>The test confirms that class distributions are <strong>not independent across years</strong> — vegetation structure has changed significantly over time.</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>LIVE (Obs)</strong></th> <th><strong>LIVE (Exp)</strong></th> <th><strong>DEAD (Obs)</strong></th> <th><strong>DEAD (Exp)</strong></th> <th><strong>BARE (Obs)</strong></th> <th><strong>BARE (Exp)</strong></th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>14035</td> <td>17810.18</td> <td>41409</td> <td>35921.25</td> <td>70185</td> <td>71897.57</td> </tr> <tr> <td>2016</td> <td>17560</td> <td>17810.18</td> <td>38873</td> <td>35921.25</td> <td>69196</td> <td>71897.57</td> </tr> <tr> <td>2018</td> <td>18983</td> <td>17880.21</td> <td>34204</td> <td>36062.50</td> <td>72936</td> <td>72180.29</td> </tr> <tr> <td>2020</td> <td>21241</td> <td>17880.21</td> <td>31112</td> <td>36062.50</td> <td>73770</td> <td>72180.29</td> </tr> <tr> <td>2022</td> <td>17442</td> <td>17880.21</td> <td>34432</td> <td>36062.50</td> <td>74249</td> <td>72180.29</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h2 id="conclusions"><strong>Conclusions</strong></h2> <ul> <li>Class balancing significantly improved DEAD prediction</li> <li>The model generalizes well across years, both visually and statistically</li> <li>NDVI remains the most valuable signal for distinguishing LIVE and BARE</li> <li>Predicted maps reflect meaningful ecological shifts — particularly post-2020 regrowth</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Incorporate 3×3 or 5×5 patch features to provide spatial context</li> <li>Analyze temporal transitions such as LIVE → DEAD or DEAD → BARE</li> <li>Investigate spatial drift between years by comparing prediction shifts in fixed coordinates</li> <li>Validate results with external field survey data or high-res canopy maps</li> <li>Expand the labeling dataset to include denser forest sections for generalization</li> </ul> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Evaluating NDVI Based Tree Classification and Label Efficiency</title><link href="https://deveshparagiri.github.io/blog/2025/label-efficiency-and-generalization-copy/" rel="alternate" type="text/html" title="Evaluating NDVI Based Tree Classification and Label Efficiency"/><published>2025-05-26T20:00:00+00:00</published><updated>2025-05-26T20:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/label-efficiency-and-generalization%20copy</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/label-efficiency-and-generalization-copy/"><![CDATA[<hr/> <p><br/></p> <h2 id="overview"><strong>Overview</strong></h2> <p>This experiment investigates how accurately I can classify individual pixels in aerial imagery as <strong>LIVE</strong>, <strong>DEAD</strong>, or <strong>BARE</strong> ground using a small number of labeled examples. I evaluate how performance scales with label count, examine the benefit of including <strong>NDVI</strong> as an explicit input feature, and explore how well a model trained on <strong>multi-year data</strong> generalizes across time.</p> <p>I aim to answer the following core questions:</p> <ol> <li>How many labeled pixels per class are needed to achieve high accuracy?</li> <li>Does adding NDVI improve model stability and performance in low-data regimes?</li> <li>Can a model trained on a single year generalize to another year?</li> <li>How well does a model trained on labeled data from all years perform when evaluated across time?</li> </ol> <hr/> <p><br/></p> <h2 id="data-and-setup"><strong>Data and Setup</strong></h2> <ul> <li>All pixels were extracted from spectrally and spatially normalized NAIP imagery (2014–2022).</li> <li>Manual labels were created by visually inspecting .png previews across years for each sampled coordinate.</li> <li>Each labeled pixel was represented using either: <ul> <li><strong>4-band spectral input</strong>: Red, Green, Blue, NIR</li> <li><strong>5-band input</strong>: Red, Green, Blue, NIR, NDVI</li> </ul> </li> </ul> <p>The model used in all experiments is a RandomForestClassifier from sklearn.</p> <hr/> <p><br/></p> <h2 id="experiment-1-accuracy-vs-label-count-with-and-without-ndvi"><strong>Experiment 1: Accuracy vs Label Count (With and Without NDVI)</strong></h2> <p>I trained the classifier using only labeled pixels from <strong>2014</strong>. For each value of k (samples per class), I ran 5 randomized 80/20 train-test splits and recorded mean accuracy and standard deviation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/accuracy_simple.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/accuracy_simple.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Accuracy vs Label Count (Without NDVI)</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/accuracy_ndvi.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/accuracy_ndvi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Accuracy vs Label Count (With NDVI)</figcaption> </figure> </div> </div> <h3 id="results-summary"><strong>Results Summary</strong></h3> <table> <thead> <tr> <th><strong>Labeled/Class (k)</strong></th> <th><strong>Accuracy (w/o NDVI)</strong></th> <th><strong>Accuracy (w/ NDVI)</strong></th> </tr> </thead> <tbody> <tr> <td>5</td> <td>~52% ± high variance</td> <td>~74% ± lower variance</td> </tr> <tr> <td>8</td> <td>~65%</td> <td>~77%</td> </tr> <tr> <td>13</td> <td>~75%</td> <td><strong>~85%</strong></td> </tr> </tbody> </table> <p>In the first experiment, I found that adding NDVI to the input significantly improves model performance, especially at low sample counts. <br/></p> <h5 id="key-takeaways"><strong>Key Takeaways:</strong></h5> <ul> <li>NDVI significantly boosts accuracy, especially at <strong>low sample counts</strong></li> <li>Including NDVI stabilizes model performance across random splits</li> <li>Without NDVI, the model struggles to distinguish classes under limited supervision</li> <li>With NDVI, LIVE pixels are often learned with high confidence even with 5–6 examples</li> </ul> <hr/> <p><br/></p> <h2 id="experiment-2-cross-year-generalization-train-on-2014--test-on-2020"><strong>Experiment 2: Cross-Year Generalization (Train on 2014 → Test on 2020)</strong></h2> <p>I trained a model on all 2014-labeled pixels (using 5-band input with NDVI) and tested it directly on labeled pixels from 2020 — no retraining or adaptation.</p> <p><strong>Results:</strong></p> <ul> <li><strong>Overall Accuracy</strong>: <strong>78.4%</strong></li> <li><strong>LIVE</strong> class had high precision and recall</li> <li><strong>BARE</strong> was consistently predicted correctly (recall = 1.0), though with some over-prediction</li> <li><strong>DEAD</strong> remained harder to capture (recall = 0.53)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Classification Report:
              precision    recall  f1-score
    BARE         0.571     1.000     0.727
    DEAD         0.818     0.529     0.643
    LIVE         0.885     0.885     0.885
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/confusion_2014to2020.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/confusion_2014to2020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="implications"><strong>Implications</strong></h2> <ul> <li><strong>NDVI improves generalization</strong> by providing a vegetation-specific signal that remains valid across years.</li> <li>The model is most confident on <strong>LIVE</strong> pixels, suggesting that greenness + NDVI are strong predictors.</li> <li><strong>DEAD</strong> and <strong>BARE</strong> are more difficult to distinguish — these likely require: <ul> <li>More training samples</li> <li>Spatial or temporal context (e.g., adjacent pixels, change over time)</li> </ul> </li> </ul> <hr/> <p><br/></p> <h2 id="conclusions"><strong>Conclusions</strong></h2> <ul> <li>Based on this experiment, with as few as <strong>10–13 labeled pixels per class</strong>, we can reach <strong>&gt;85% accuracy</strong> using NDVI.</li> <li>Training on one year and applying to another is feasible if the data is normalized.</li> <li>NDVI should be included as a feature — it boosts performance significantly and reduces label burden.</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Increase label count across years</li> <li>Test reverse generalization: 2020 → 2014</li> <li>Predict full raster maps using trained models</li> <li>Potentially Add 3×3 or 5×5 patch-based context around each pixel</li> </ul> <hr/> <p><br/></p> <h2 id="experiment-3-combined-year-generalization-and-evaluation"><strong>Experiment 3: Combined-Year Generalization and Evaluation</strong></h2> <p>In this experiment, I explored how well a model trained on <strong>labeled pixels from all years combined</strong> performs across time. Rather than training on a single year, I sampled <code class="language-plaintext highlighter-rouge">k = 10–25</code> pixels per class (LIVE, DEAD, BARE) from the full labeled dataset and tested the model in two complementary ways:</p> <ol> <li><strong>Per-Year Generalization:</strong> Test accuracy is measured individually on each year (2014–2022).</li> <li><strong>80/20 Mixed-Year Accuracy:</strong> A stratified 80/20 split is used across all labeled data, simulating a more randomized evaluation.</li> </ol> <hr/> <p><br/></p> <h3 id="label-distribution"><strong>Label Distribution</strong></h3> <p>Before running this experiment, I reviewed how labeled samples were distributed across years and classes:</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>LIVE</strong></th> <th><strong>DEAD</strong></th> <th><strong>BARE</strong></th> <th><strong>Label Count Range (max - min)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>2014</strong></td> <td>19</td> <td>19</td> <td>13</td> <td>6</td> </tr> <tr> <td><strong>2016</strong></td> <td>30</td> <td>13</td> <td>8</td> <td>22</td> </tr> <tr> <td><strong>2018</strong></td> <td>15</td> <td>12</td> <td>24</td> <td>12</td> </tr> <tr> <td><strong>2020</strong></td> <td>26</td> <td>17</td> <td>8</td> <td>18</td> </tr> <tr> <td><strong>2022</strong></td> <td>16</td> <td>16</td> <td>19</td> <td>3</td> </tr> <tr> <td><strong>Total</strong></td> <td><strong>106</strong></td> <td><strong>77</strong></td> <td><strong>72</strong></td> <td> </td> </tr> </tbody> </table> <hr/> <p>While the class imbalance isn’t extreme, some years (like 2016 and 2020) had disproportionately more LIVE samples than BARE or DEAD. This may partially explain performance variation across years.</p> <hr/> <p><br/></p> <h3 id="graph-1-accuracy-per-year-trained-on-all-years"><strong>Graph 1: Accuracy per Year (Trained on All Years)</strong></h3> <p>I trained the model on all years using k samples per class and evaluated it year-by-year to assess generalization over time. <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/generalized_all.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/generalized_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Generalized Model applied to each year</figcaption> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Accuracy improves with label count but flattens after ~20 samples/class.</li> <li><strong>2022 consistently outperformed other years</strong>, reaching over 90% accuracy.</li> <li>2014 and 2016 showed slightly lower accuracy, likely due to noisier labels or less distinctive spectral features.</li> </ul> <hr/> <p><br/></p> <h3 id="graph-2-accuracy-vs-k-8020-random-split"><strong>Graph 2: Accuracy vs k (80/20 Random Split)</strong></h3> <p>For comparison, I also performed a standard 80/20 train-test split on the full dataset. <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/generalized_8020.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/generalized_8020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Generalized Model Accuracy vs k</figcaption> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Accuracy was more variable at lower k values due to randomness in class composition.</li> <li>With 20+ samples per class, performance stabilized and closely matched the per-year evaluation curve.</li> </ul> <hr/> <p><br/></p> <h3 id="confusion-matrices-per-year"><strong>Confusion Matrices per Year</strong></h3> <p>To better understand how each class was predicted over time, I generated confusion matrices for each year using the model trained on all data (k=25 per class). <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/confusion_all.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/confusion_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix for Generalized Model Applied to Each Year</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="per-year-breakdown"><strong>Per-Year Breakdown</strong></h3> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>Accuracy</strong></th> <th><strong>F1 (LIVE)</strong></th> <th><strong>F1 (DEAD)</strong></th> <th><strong>F1 (BARE)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>2014</strong></td> <td>0.837</td> <td>0.778</td> <td>0.878</td> <td>0.741</td> </tr> <tr> <td><strong>2016</strong></td> <td>0.867</td> <td>0.778</td> <td>0.923</td> <td>0.667</td> </tr> <tr> <td><strong>2018</strong></td> <td>0.898</td> <td>0.786</td> <td>0.846</td> <td>0.957</td> </tr> <tr> <td><strong>2020</strong></td> <td>0.875</td> <td>0.846</td> <td>0.903</td> <td>0.778</td> </tr> <tr> <td><strong>2022</strong></td> <td><strong>0.901</strong></td> <td>0.786</td> <td>0.824</td> <td><strong>1.000</strong></td> </tr> </tbody> </table> <hr/> <p><br/></p> <h3 id="analysis-and-hypotheses"><strong>Analysis and Hypotheses</strong></h3> <ul> <li><strong>LIVE pixels were consistently learned well</strong> across all years, with F1 scores between 0.77–0.85.</li> <li><strong>BARE improved sharply in later years</strong>, especially in 2022 where it reached perfect precision and recall.</li> <li><strong>DEAD remained the most ambiguous</strong>, frequently confused with both LIVE and BARE. Its spectral profile is more variable and likely requires additional temporal or spatial information.</li> <li><strong>2022 performed best</strong>, possibly due to: <ul> <li>Balanced label distribution across all classes (All three classes are within a 3-count range)</li> <li>Better image quality or spectral separation</li> <li>Fewer mislabels from manual annotation</li> </ul> </li> </ul> <blockquote> <p class="block-tip">The generalized model’s consistent performance across years confirms that the <strong>spatial and spectral normalization approach</strong> was effective.</p> </blockquote> <p><br/></p> <h3 id="how-many-labels-are-enough"><strong>How Many Labels Are Enough?</strong></h3> <p>From all experiments, I observed that performance gains are <strong>nonlinear</strong> with respect to label count (logarithmic):</p> <ul> <li>The steepest gains occur between 5 and 15 samples/class.</li> <li>Accuracy improvements <strong>flatten beyond ~20 samples/class</strong>, suggesting diminishing returns.</li> <li>For robust, multi-year generalization: <ul> <li><strong>20–25 samples/class</strong> per year is ideal</li> <li>Alternatively, ~100 well-distributed samples/class across years can generalize effectively</li> </ul> </li> </ul> <hr/> <p><br/></p> <h3 id="takeaways"><strong>Takeaways</strong></h3> <ul> <li>Combined-year training leads to a <strong>stable, high-performing model</strong> across time.</li> <li><strong>NDVI continues to be critical</strong>, especially for identifying LIVE vegetation.</li> <li><strong>Confusion patterns reveal that DEAD remains the weakest class</strong>, needing more contextual signals.</li> <li>Training on all available data provides stronger generalization than per-year splits or single-year baselines.</li> </ul> <hr/> <p><br/></p> <h3 id="next-steps-1"><strong>Next Steps</strong></h3> <ul> <li>Increase the number of labeled pixels per class to 30–40 to further reduce variance.</li> <li>Investigate spatial context by incorporating patch-level features (e.g., 3×3 or 5×5 neighborhoods).</li> </ul> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Detecting Tree Mortality from Aerial Imagery</title><link href="https://deveshparagiri.github.io/blog/2025/research-logs/" rel="alternate" type="text/html" title="Detecting Tree Mortality from Aerial Imagery"/><published>2025-05-18T20:00:00+00:00</published><updated>2025-05-18T20:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/research-logs</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/research-logs/"><![CDATA[<hr/> <p><br/></p> <h2 id="motivation"><strong>Motivation</strong></h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/maingraph.png" sizes="95vw"/> <img src="/assets/img/research_log/maingraph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"></figcaption> </figure> </div> </div> <p>The sharp divergence between ED-Lidar reconstructions, Landsat NDVI, and AmeriFlux GPP observations (see figure) around 2010–2020 sparked this investigation. While historical reconstructions suggest stable or even rising productivity, both satellite vegetation indices (NDVI) and flux tower data show a clear decline in GPP at the US-MPJ site during this period.</p> <p>This discrepancy raised a key question: Is this ecosystem experiencing large-scale tree mortality that is not being captured by traditional models?</p> <p>To answer this, we developed an image-based approach to directly detect vegetation loss and tree death from aerial imagery, aiming to complement and explain these broader ecosystem signals.</p> <p><br/></p> <h2 id="log-1-deepforest-approach"><strong>Log 1: DeepForest Approach</strong></h2> <p><br/></p> <h3 id="research-question"><strong>Research Question</strong></h3> <p>How can we reliably detect <strong>tree mortality</strong> across time using <strong>aerial imagery</strong>? Can <strong>pretrained object detection models</strong>, like <strong>DeepForest</strong>, give us useful indicators of <strong>tree health or death</strong>?</p> <hr/> <p><br/></p> <h3 id="phase-1-approach--using-deepforest-for-crown-detection"><strong>Phase 1 Approach — Using DeepForest for Crown Detection</strong></h3> <p>DeepForest is a state-of-the-art deep learning model trained on RGB imagery to detect <strong>individual tree crowns</strong>. It outputs <strong>bounding boxes</strong> around tree-like objects, which initially seemed promising for analyzing:</p> <ul> <li><strong>Tree count changes over time</strong></li> <li><strong>Tree canopy shrinkage</strong></li> <li>Mortality via <strong>absence or degradation</strong> of detected crowns</li> </ul> <hr/> <p><br/></p> <h3 id="thought-process"><strong>Thought Process</strong></h3> <p>Use a pretrained model to:</p> <ul> <li>Quickly extract structured detections</li> <li>Use <strong>bounding box count or size</strong> as a proxy for forest density</li> <li>Detect <strong>tree loss trends over time</strong> without training from scratch</li> </ul> <hr/> <p><br/></p> <h3 id="methodology"><strong>Methodology</strong></h3> <ul> <li><strong>Imagery Source:</strong> NAIP (2014–2022), 512×512 RGB tiles, resolution: 1.0m → 0.6m</li> <li><strong>Tool:</strong> <a href="https://github.com/weecology/DeepForest"><code class="language-plaintext highlighter-rouge">DeepForest</code></a></li> <li><strong>Workflow:</strong> <ul> <li>Load NAIP tile</li> <li>Predict bounding boxes using pretrained model</li> <li>Compare box count and coverage across years</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepforest</span> <span class="kn">import</span> <span class="n">main</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">main</span><span class="p">.</span><span class="nf">deepforest</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">use_release</span><span class="p">()</span>
<span class="n">boxes</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_image</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">naip_tile.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <hr/> <p><br/></p> <h3 id="sample-outputs"><strong>Sample Outputs</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log1.png" sizes="95vw"/> <img src="/assets/img/research_log/log1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepForest detection results before parameter tuning</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log1-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log1-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepForest detection results after parameter tuning</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="challenges-encountered"><strong>Challenges Encountered</strong></h3> <p><br/></p> <h5 id="1-spatial-resolution-mismatch">1. <strong>Spatial Resolution Mismatch</strong></h5> <ul> <li>DeepForest was trained on ~0.1m resolution. NAIP tiles at 1.0m/0.6m were <strong>too coarse</strong>.</li> <li>Crowns were <strong>blurry</strong>, often <strong>undetected or merged</strong> into clusters.</li> </ul> <h5 id="2-bounding-boxes--tree-area">2. <strong>Bounding Boxes ≠ Tree Area</strong></h5> <ul> <li>Boxes were <strong>not calibrated</strong> to actual canopy area.</li> <li>Detection size and count varied erratically due to visual artifacts.</li> </ul> <h5 id="3-noisy-temporal-consistency">3. <strong>Noisy Temporal Consistency</strong></h5> <ul> <li>Detections fluctuated due to shadows, season, sun angle — <strong>not ecological change</strong>.</li> <li>Trees “disappeared” or “reappeared” randomly between years.</li> </ul> <hr/> <p><br/></p> <h3 id="summary"><strong>Summary</strong></h3> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Visual Interpretability</td> <td>❌ Low</td> </tr> <tr> <td>Spatial Precision</td> <td>❌ Weak</td> </tr> <tr> <td>Temporal Consistency</td> <td>❌ Unusable</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h3 id="key-takeaways"><strong>Key Takeaways</strong></h3> <ul> <li>Pretrained models are <strong>domain-constrained</strong></li> <li>Tree detection ≠ mortality inference</li> <li><strong>RGB-only features are too volatile</strong> over time</li> </ul> <hr/> <p><br/></p> <h3 id="transition-to-next-phase"><strong>Transition to Next Phase</strong></h3> <p>We needed a strategy focused on <strong>semantic labeling</strong> (LIVE / DEAD / BARE), not detection. This led to the <strong>5x5 patch-based classification</strong> approach — covered next in <strong>Log 2</strong>.</p> <hr/> <p><br/></p> <h2 id="log-2-patch-based-classification"><strong>Log 2: Patch-Based Classification</strong></h2> <p><br/></p> <h3 id="research-goal"><strong>Research Goal</strong></h3> <p>Label small regions of aerial imagery based on <strong>ecological intuition</strong>, using spatial patches instead of pixel-level or bounding box classification.</p> <hr/> <p><br/></p> <h3 id="phase-2-approach--55-patch-labeling-using-human-intuition"><strong>Phase 2 Approach — 5×5 Patch Labeling Using Human Intuition</strong></h3> <p>Each 512×512 tile was divided into a 5×5 grid (i.e. ~25×25 pixel patches). Each patch was visually labeled based on <strong>dominant appearance</strong>: LIVE, DEAD, or BARE.</p> <hr/> <p><br/></p> <h3 id="thought-process-1"><strong>Thought Process</strong></h3> <ul> <li>Smoother than pixel classification</li> <li>Easier than labeling full tiles</li> <li>Provides enough context for human labeling (e.g., sparse vs dense canopy)</li> </ul> <hr/> <p><br/></p> <h3 id="methodology-1"><strong>Methodology</strong></h3> <h5 id="1-patch-generation">1. <strong>Patch Generation</strong></h5> <ul> <li>Each tile → ~100+ 5×5 patches</li> <li>Recorded in <code class="language-plaintext highlighter-rouge">valid_patches.csv</code>: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r329_c58_y2020.png, 329, 58, 2020, , possibly BARE or DEAD
</code></pre></div> </div> </li> </ul> <h5 id="2-visual-labeling">2. <strong>Visual Labeling</strong></h5> <ul> <li>No NDVI or thresholding</li> <li>Labels were assigned via: <ul> <li>Manual inspection across years</li> <li>Texture, color, and shape</li> <li>“Hint” labels updated during review</li> </ul> </li> </ul> <h5 id="3-model-training">3. <strong>Model Training</strong></h5> <ul> <li>Feature extraction: <ul> <li>RGB stats (mean, std)</li> <li>Optional raw pixel flattening</li> </ul> </li> <li>Classifier: <ul> <li>Random Forest (baseline)</li> <li>Small MLP (later)</li> </ul> </li> </ul> <hr/> <p><br/></p> <h3 id="sample-outputs-1"><strong>Sample Outputs</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-1.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Sample patch labeling</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Area of Interest</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-3.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Dead Tree Area Over Time</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-4.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Transition Matrices for 5x5 Patch Classification</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="challenges-encountered-1"><strong>Challenges Encountered</strong></h3> <p><br/></p> <h5 id="1-labeling-conflicts-majority-dilemma">1. <strong>Labeling Conflicts (Majority Dilemma)</strong></h5> <ul> <li>Mixed-content patches (e.g., half LIVE, half DEAD)</li> <li>Label assignment was subjective → high variance</li> </ul> <h5 id="2-spectral-inconsistency">2. <strong>Spectral Inconsistency</strong></h5> <ul> <li>Same class in different years looked different (due to sensors, lighting)</li> <li>Model trained on one year <strong>couldn’t generalize</strong> to another</li> </ul> <h5 id="3-spatial-drift">3. <strong>Spatial Drift</strong></h5> <ul> <li>Misalignments between years → same patch ID pointed to <strong>different physical areas</strong></li> <li>Resolution changes (1m vs 0.6m) broke equivalence</li> </ul> <hr/> <p><br/></p> <h3 id="summary-1"><strong>Summary</strong></h3> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>In-year Accuracy</td> <td>~65–70%</td> </tr> <tr> <td>Cross-year Generalization</td> <td>❌ Failed (&lt;50%)</td> </tr> <tr> <td>Label Noise</td> <td>High</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h3 id="key-takeaways-1"><strong>Key Takeaways</strong></h3> <ul> <li><strong>Visual labeling ≠ repeatable</strong> at scale</li> <li>Patches blur meaningful distinctions</li> <li>Spectral models must <strong>anchor on spatial precision</strong></li> <li>RGB features alone cannot reliably detect long-term changes</li> </ul> <hr/> <p><br/></p> <h3 id="transition-to-next-phase-1"><strong>Transition to Next Phase</strong></h3> <p>To resolve both spectral and spatial instability, we transitioned to <strong>pixel-level temporal modeling</strong> — tracking <strong>individual pixels across all years</strong>. That’s the focus of <strong>Log 3</strong>.</p> <hr/> <p><br/></p> <h2 id="log-3-single-pixel-temporal-classification"><strong>Log 3: Single Pixel Temporal Classification</strong></h2> <p><br/></p> <h3 id="research-goal-1"><strong>Research Goal</strong></h3> <p>Track <strong>individual pixels</strong> over time and use their <strong>temporal NDVI trajectories</strong> to classify them into ecological categories (LIVE, DEAD, BARE).</p> <hr/> <p><br/></p> <h3 id="phase-3-approach--per-pixel-temporal-ndvi-classifier"><strong>Phase 3 Approach — Per-Pixel Temporal NDVI Classifier</strong></h3> <p>We moved away from patches and bounding boxes entirely. Each pixel became its own data point, tracked across all available years.</p> <hr/> <p><br/></p> <h3 id="thought-process-2"><strong>Thought Process</strong></h3> <ul> <li>Control exact <strong>spatial location</strong></li> <li>Use <strong>temporal signal</strong> as primary feature (e.g., NDVI over time)</li> <li>Detect transitions like LIVE → DEAD or DEAD → BARE</li> <li>Label using <strong>human-in-the-loop hints</strong> supported by NDVI</li> </ul> <hr/> <p><br/></p> <h3 id="methodology-2"><strong>Methodology</strong></h3> <h5 id="1-pixel-extraction">1. <strong>Pixel Extraction</strong></h5> <ul> <li>Spatially aligned tiles across years (2014–2022)</li> <li>Pixels indexed by <code class="language-plaintext highlighter-rouge">row</code>, <code class="language-plaintext highlighter-rouge">col</code></li> <li> <p>Each pixel assigned time-series NDVI values:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>filename,row,col,year,ndvi,label_hint
r327_c59_y2020.png,327,59,2020,0.162,"possibly BARE or DEAD"

</code></pre></div> </div> </li> </ul> <h5 id="2-labeling-strategy">2. <strong>Labeling Strategy</strong></h5> <ul> <li>Combined: <ul> <li>Visual inspection across years</li> <li>NDVI pattern (e.g., drop then flatten)</li> <li>Human-annotated hints like “likely DEAD”</li> </ul> </li> </ul> <h5 id="3-model-input">3. <strong>Model Input</strong></h5> <ul> <li>Each training sample: NDVI vector across years <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0.58 (2014), 0.56 (2016), 0.44 (2018), 0.23 (2020), 0.22 (2022)]
→ label: DEAD
</code></pre></div> </div> </li> </ul> <hr/> <p><br/></p> <h3 id="sample-outputs-2"><strong>Sample Outputs</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/raw_buffer_2014.png" sizes="95vw"/> <img src="/assets/img/research_log/raw_buffer_2014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Raw Buffer 2014</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/matched_buffer_2014.png" sizes="95vw"/> <img src="/assets/img/research_log/matched_buffer_2014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Spectrally Matched Buffer 2014</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log3-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log3-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Vegetation Reflectance Drift</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log3-1.png" sizes="95vw"/> <img src="/assets/img/research_log/log3-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Single Pixel Time Series</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="key-strengths"><strong>Key Strengths</strong></h3> <ul> <li>True <strong>temporal stability</strong> — fixed pixel over time</li> <li>Model can <strong>learn transitions</strong>, not just snapshot classes</li> <li>Eliminates resolution-induced mapping drift</li> </ul> <hr/> <p><br/></p> <h3 id="experimental-insights"><strong>Experimental Insights</strong></h3> <ul> <li>Temporal modeling removes much spatial noise</li> <li>NDVI change patterns (drop + flatten) correlate well with true mortality</li> <li>Model interpretability improves (you can “see” what happened)</li> </ul> <hr/> <p><br/></p> <h3 id="current-status"><strong>Current Status</strong></h3> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Temporal NDVI consistency</td> <td>✅ Strong</td> </tr> <tr> <td>Human label quality</td> <td>✅ High confidence</td> </tr> <tr> <td>Class balance</td> <td>⚠️ Still tuning</td> </tr> <tr> <td>Next step</td> <td>Expand labeled samples &amp; train temporal classifier</td> </tr> </tbody> </table> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">AI Agent vs Workflow</title><link href="https://deveshparagiri.github.io/blog/2025/agent-vs-workflow-copy/" rel="alternate" type="text/html" title="AI Agent vs Workflow"/><published>2025-03-26T20:00:00+00:00</published><updated>2025-03-26T20:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/agent-vs-workflow%20copy</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/agent-vs-workflow-copy/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <p><br/></p> <p>AI agents are all the hype. 2025 is touted to be the year of agents. Some say they’ll take over all our jobs; others call them an over-engineered gimmick. Regardless of where you stand, one thing is clear: <strong>agents are here to stay</strong>, and they’re poised to reshape how developers build and interact with software.</p> <p>But before you jump on the hype train, it’s worth asking – <em>what actually sets agents apart from traditional AI workflows?</em> And when should you use one over another?</p> <p>This blog breaks down the technical differences between agents and workflows – just the key details with implications for your tech stack.</p> <p><br/></p> <h2 id="agents-vs-workflows---tldr"><strong>Agents vs Workflows - TLDR</strong></h2> <p><br/></p> <table> <thead> <tr> <th><strong>Feature</strong></th> <th><strong>AI Agent</strong></th> <th><strong>AI Workflow</strong></th> </tr> </thead> <tbody> <tr> <td>Core Idea</td> <td>Autonomous Entity with goals + reasoning</td> <td>Perform a defined sequence of tasks</td> </tr> <tr> <td>State</td> <td>Has memory, state, and internal feedback loops</td> <td>Usually static or explicit state</td> </tr> <tr> <td>Flexibility</td> <td>Dynamic, pivots according to the requirement</td> <td>Predefined, changes require editing the flow</td> </tr> <tr> <td>Determinism</td> <td>Non-deterministic; same prompt ≠ same output</td> <td>Deterministic (mostly)</td> </tr> <tr> <td>Sample use-case</td> <td>Open-ended tasks, multiple steps, vague goals</td> <td>Structured, repeated, and known flows</td> </tr> </tbody> </table> <p><br/></p> <h2 id="so-what-is-an-agent"><strong>So what is an Agent?</strong></h2> <p><br/> Think of an agent as your mini-employee with a brain, a set of tasks, and access to tools.</p> <p>You give it a goal and it figures out how to get there – which tools to use, questions to ask, and maybe even ask clarifying questions. The agent has full autonomy every step of the way, including the output.</p> <p>An agent can evaluate it’s own response and choose to restart it’s task. Effectively, the agent can optimize it’s own processes.</p> <p><br/></p> <h4 id="core-behaviors"><strong>Core Behaviors:</strong></h4> <ul> <li><strong>Goal-oriented:</strong> You say “Summarize this repo and build a README,” it plans its own steps.</li> <li><strong>Tool-using:</strong> Calls APIs, scrapes data, runs subprocesses — whatever gets the job done.</li> <li><strong>Memory-aware:</strong> Keeps track of prior steps, retries intelligently.</li> <li><strong>Autonomous (ish):</strong> Can decide its own flow, within guardrails.</li> </ul> <p><br/></p> <h3 id="sample-agent-in-action"><strong>Sample Agent in Action</strong></h3> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/agent_vs_workflow/asset.webp" sizes="95vw"/> <img src="/assets/img/agent_vs_workflow/asset.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">AI Agent Decision Flow Diagram. Source: <a href="https://www.anthropic.com/engineering/building-effective-agents">Anthropic</a></figcaption> </figure> </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span>
    <span class="n">role</span><span class="o">=</span><span class="sh">"</span><span class="s">Junior Dev</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">search_docs</span><span class="p">,</span> <span class="n">write_code</span><span class="p">,</span> <span class="n">run_tests</span><span class="p">,</span> <span class="n">evaluate_code</span><span class="p">,</span> <span class="n">human_input</span><span class="p">],</span>
    <span class="n">memory</span><span class="o">=</span><span class="nc">Memory</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="sh">"</span><span class="s">Create Quarto Docs for this repo.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h2 id="so-what-is-a-workflow"><strong>So what is a Workflow?</strong></h2> <p><br/></p> <p>A <strong>workflow</strong> is more like a flowchart. You lay out the steps: “Do A, then B, then C.” It’s deterministic, composable, and rock-solid for anything structured.</p> <p>Think CI/CD pipelines, document processing chains, or RAG pipelines.</p> <p><br/></p> <h4 id="core-behaviors-1"><strong>Core Behaviors:</strong></h4> <ul> <li><strong>Step-by-step logic:</strong> The flow is explicit and predictable.</li> <li><strong>Composable:</strong> You can chain blocks together easily.</li> <li><strong>Debuggable:</strong> You know exactly where something breaks.</li> <li><strong>Efficient:</strong> Less overhead, faster runtime.</li> </ul> <p><br/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="p">{</span><span class="sh">"</span><span class="s">docs</span><span class="sh">"</span><span class="p">:</span> <span class="n">retriever</span><span class="p">,</span> <span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">]}</span>
    <span class="o">|</span> <span class="p">{</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">:</span> <span class="n">rag_chain</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h2 id="what-about-a-hybrid"><strong>What about a hybrid?</strong></h2> <p><br/></p> <p>Some of the best systems mix both. You could use a workflow to spawn agents, or let agents call reliable AI workflows. For example:</p> <ul> <li>AI agent for making a landing page calls several workflows - outline generator, code generator, deployment workflow</li> </ul> <p>You get the flexibility of agents without compromising on the reliability of workflows.</p> <p><br/></p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p><br/></p> <p>Agents are <em>not</em> here to replace workflows. They’re here to unlock new kinds of problems we couldn’t automate before.</p> <p>Use <strong>agents</strong> when you want flexible, autonomous decision-making.</p> <p>Use <strong>workflows</strong> when you want reliable, fast, auditable pipelines.</p> <p>Sometimes you need brains. Sometimes you need structure. And sometimes — you need both.</p> <p><br/></p> <h2 id="helpful-links-and-resources"><strong>Helpful Links and Resources</strong></h2> <p><br/></p> <ul> <li><a href="https://www.youtube.com/watch?v=tx5OapbK-8A">Build your first agent</a></li> <li><a href="https://www.youtube.com/watch?v=4nZl32FwU-o">Multi-agent architectures</a></li> <li><a href="https://www.langflow.org/">Low-code agent builder</a></li> <li><a href="https://github.com/e2b-dev/awesome-ai-agents">AI Agent Repository</a></li> </ul>]]></content><author><name></name></author><category term="code"/><category term="ai"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Deploying a node.JS app to AWS EC2</title><link href="https://deveshparagiri.github.io/blog/2024/deploy-nodejs/" rel="alternate" type="text/html" title="Deploying a node.JS app to AWS EC2"/><published>2024-11-20T15:59:00+00:00</published><updated>2024-11-20T15:59:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2024/deploy-nodejs</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2024/deploy-nodejs/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <p>You’ve successfully built your application, and now you’re looking forward to deploying it! This blog covers the basics of <strong>AWS EC2</strong>, setup, and deployment. <br/> <br/></p> <h2 id="setup-an-aws-ec2-instance"><strong>Setup an AWS EC2 Instance</strong></h2> <p><br/></p> <h3 id="what-even-is-ec2"><strong>What even is EC2?</strong></h3> <p><strong>EC2</strong> stands for <strong>Elastic Cloud Compute</strong> and is a virtual machine where you can deploy your applications. What’s so cool about it? It allows for easy <strong>scaling</strong> to handle more requests coming into your application when the need arises, making it easy to handle varying demand. It’s also quite versatile, making it a great choice for developers.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/balancer.png" sizes="95vw"/> <img src="/assets/img/blog1/balancer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h3 id="why-are-we-using-it"><strong>Why are we using it?</strong></h3> <p>It’s simple, ease of use, and quick setup makes it a great first choice for those getting started with development. Above all, it’s cheap because of <strong>“on-demand” scaling</strong> helping to only pull out the big guns when absolutely required. Alternatively, we could also use an <strong>Azure VM</strong> or the <strong>Google Cloud Engine (GCE)</strong> to deploy our app, which I will cover in later blogs.</p> <p><br/></p> <h3 id="getting-started"><strong>Getting Started</strong></h3> <p>Let’s set up the AWS instance and connect to it via <strong>SSH</strong>.</p> <h4 id="1-login-to-your-aws-account-root-user">1. Login to your AWS account (Root User)</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/awslogin.png" sizes="95vw"/> <img src="/assets/img/blog1/awslogin.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h4 id="2-launching-an-instance">2. Launching an instance</h4> <p>Under the <strong>EC2</strong>, choose <strong>launch instance</strong>. For this application, we will use <strong>AWS Linux</strong> as our OS Image due to its tight integration with AWS and ease of use. Alternatively, you could also use <strong>Ubuntu</strong> but will have to make some changes (<code class="language-plaintext highlighter-rouge">apt-get</code> vs. <code class="language-plaintext highlighter-rouge">yum</code>) for the next steps during installing dependencies.</p> <p>Choose <strong>t2.micro</strong> for instance type – simplest instance, and is very cost effective. If your application is quite large and compute intensive, shift to <strong>t2.medium</strong> for ensuring enough memory capacity.</p> <p>We create a new <strong>key pair</strong>, which will enable us to connect to our instance through <strong>SSH</strong> and get started with setting up. A <code class="language-plaintext highlighter-rouge">.pem</code> file will be downloaded to your local machine.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/awskeypair.png" sizes="95vw"/> <img src="/assets/img/blog1/awskeypair.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We allow all traffic, so we can access the instance from anywhere. We will be adding some inbound rules later on to make our port accessible to the public DNS.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/awsnetwork.png" sizes="95vw"/> <img src="/assets/img/blog1/awsnetwork.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h4 id="3-connecting-to-instance-ssh">3. Connecting to Instance (SSH)</h4> <p>To connect, you must first download the <code class="language-plaintext highlighter-rouge">.pem</code> file from when you set up the EC2 instance. Execute the below commands.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>400 &lt;path/to/security-key.pem&gt; <span class="c">#read permissions</span>

ssh <span class="nt">-i</span> &lt;path/to/security-key.pem&gt; ec2-user@ec2-&lt;Ipv4 Address&gt;.compute.amazonaws.com
</code></pre></div></div> <p><br/></p> <h2 id="installing-dependencies"><strong>Installing Dependencies</strong></h2> <p>The dependencies we will require include:</p> <ul> <li>node</li> <li>git</li> <li>nginx</li> <li>pm2</li> </ul> <p>Ensure all packages are up-to-date</p> <p>Ensure all packages are up-to-date</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum upgrade <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>yum update
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Installing Node</span>

curl <span class="nt">-o-</span> https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash
<span class="nb">.</span> ~/.nvm/nvm.sh
nvm <span class="nb">install </span>node

node <span class="nt">-v</span>
<span class="c"># v22.9.0</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Installing and Setting up Git</span>

yum <span class="nb">install </span>git <span class="nt">-y</span>

ssh-keygen <span class="nt">-t</span> ed25519 <span class="nt">-C</span> <span class="s2">"your email"</span>
<span class="nb">eval</span> <span class="s2">"</span><span class="si">$(</span>ssh-agent <span class="nt">-s</span><span class="si">)</span><span class="s2">"</span>
ssh-add ~/.ssh/id_ed25519

<span class="nb">cat</span> ~/.ssh/id_ed25519.pub
ssh-keyscan github.com <span class="o">&gt;&gt;</span> ~/.ssh/known_hosts
ssh <span class="nt">-T</span> git@github.com

git clone &lt;Your Repository&gt;
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Installing pm2 and nginx</span>

npm <span class="nb">install</span> <span class="nt">-g</span> pm2

<span class="nb">sudo </span>yum <span class="nb">install </span>nginx <span class="nt">-y</span>
<span class="nb">sudo </span>systemctl start nginx
<span class="nb">sudo </span>systemctl status nginx
</code></pre></div></div> <p><br/></p> <h2 id="deployment"><strong>Deployment</strong></h2> <p>We will use nginx as a reverse proxy forwarding all inbound calls from port 80 to our application. For now, we will only focus on HTTP. Now, under <code class="language-plaintext highlighter-rouge">/etc/nginx/sites-available/</code> modify the <code class="language-plaintext highlighter-rouge">default</code> config.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>server <span class="o">{</span>
    listen 80<span class="p">;</span>
    server_name your-server-ip-or-domain<span class="p">;</span>

    location / <span class="o">{</span>
        proxy_pass http://127.0.0.1:3000<span class="p">;</span> <span class="c"># Replace with your app's port</span>
        proxy_http_version 1.1<span class="p">;</span>
        proxy_set_header Upgrade <span class="nv">$http_upgrade</span><span class="p">;</span>
        proxy_set_header Connection <span class="s1">'upgrade'</span><span class="p">;</span>
        proxy_set_header Host <span class="nv">$host</span><span class="p">;</span>
        proxy_cache_bypass <span class="nv">$http_upgrade</span><span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>We test the nginx config and restart to apply new changes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nginx <span class="nt">-t</span>
<span class="nb">sudo </span>systemctl restart nginx
</code></pre></div></div> <p>To learn more about nginx, refer this <a href="https://nginx.org/en/docs/beginners_guide.html">guide</a>.</p> <p>Finally, we use pm2 to ensure that the application runs continuously and restarts automatically if it crashes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pm2 start app.js <span class="nt">--name</span> <span class="s2">"my-app"</span> <span class="c"># Replace `app.js` with your main file name</span>
pm2 save

pm2 list <span class="c"># Shows if the app is running</span>
pm2 log <span class="c"># Check deployment log</span>

pm2 save <span class="c"># Save process list to enable auto-restart on server reboot</span>
pm2 startup systemd <span class="c"># Generate startup script for reboot</span>
</code></pre></div></div> <p>You should now be able to access the application through the <code class="language-plaintext highlighter-rouge">Ipv4</code> DNS of your server!</p>]]></content><author><name></name></author><category term="code"/><category term="cloud"/><summary type="html"><![CDATA[Introduction]]></summary></entry></feed>