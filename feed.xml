<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://deveshparagiri.github.io/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deveshparagiri.github.io/" rel="alternate" type="text/html" hreflang="en"/><updated>2025-05-26T22:16:54+00:00</updated><id>https://deveshparagiri.github.io/feed.xml</id><title type="html">blank</title><subtitle>Dev Paragiri&apos;s personal website. </subtitle><entry><title type="html">Evaluating NDVI Based Tree Classification and Label Efficiency</title><link href="https://deveshparagiri.github.io/blog/2025/label-efficiency-and-generalization/" rel="alternate" type="text/html" title="Evaluating NDVI Based Tree Classification and Label Efficiency"/><published>2025-05-26T20:00:00+00:00</published><updated>2025-05-26T20:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/label-efficiency-and-generalization</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/label-efficiency-and-generalization/"><![CDATA[<hr/> <p><br/></p> <h2 id="overview"><strong>Overview</strong></h2> <p>This experiment investigates how accurately I can classify individual pixels in aerial imagery as <strong>LIVE</strong>, <strong>DEAD</strong>, or <strong>BARE</strong> ground using a small number of labeled examples. I assess how performance scales with label count and evaluate the benefit of using <strong>NDVI</strong> as an explicit input feature.</p> <p>I try to answer three core questions:</p> <ol> <li>How many labeled pixels per class are needed to achieve high accuracy?</li> <li>Does adding NDVI to the input improve model performance?</li> <li>Can a model trained on one year (2014) generalize to another (2020)?</li> </ol> <hr/> <p><br/></p> <h2 id="data-and-setup"><strong>Data and Setup</strong></h2> <ul> <li>All pixels were extracted from spectrally and spatially normalized NAIP imagery (2014–2022).</li> <li>Manual labels were created by visually inspecting .png previews across years for each sampled coordinate.</li> <li>Each labeled pixel was represented using either: <ul> <li><strong>4-band spectral input</strong>: Red, Green, Blue, NIR</li> <li><strong>5-band input</strong>: Red, Green, Blue, NIR, NDVI</li> </ul> </li> </ul> <p>The model used in all experiments is a RandomForestClassifier from sklearn.</p> <hr/> <p><br/></p> <h2 id="experiment-1-accuracy-vs-label-count-with-and-without-ndvi"><strong>Experiment 1: Accuracy vs Label Count (With and Without NDVI)</strong></h2> <p>I trained the classifier using only labeled pixels from <strong>2014</strong>. For each value of k (samples per class), I ran 5 randomized 80/20 train-test splits and recorded mean accuracy and standard deviation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/accuracy_simple.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/accuracy_simple.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Accuracy vs Label Count (Without NDVI)</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/accuracy_ndvi.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/accuracy_ndvi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Accuracy vs Label Count (With NDVI)</figcaption> </figure> </div> </div> <h3 id="results-summary"><strong>Results Summary</strong></h3> <table> <thead> <tr> <th><strong>Labeled/Class (k)</strong></th> <th><strong>Accuracy (w/o NDVI)</strong></th> <th><strong>Accuracy (w/ NDVI)</strong></th> </tr> </thead> <tbody> <tr> <td>5</td> <td>~52% ± high variance</td> <td>~74% ± lower variance</td> </tr> <tr> <td>8</td> <td>~65%</td> <td>~77%</td> </tr> <tr> <td>13</td> <td>~75%</td> <td><strong>~85%</strong></td> </tr> </tbody> </table> <p>In the first experiment, I found that adding NDVI to the input significantly improves model performance, especially at low sample counts. <br/></p> <h5 id="key-takeaways"><strong>Key Takeaways:</strong></h5> <ul> <li>NDVI significantly boosts accuracy, especially at <strong>low sample counts</strong></li> <li>Including NDVI stabilizes model performance across random splits</li> <li>Without NDVI, the model struggles to distinguish classes under limited supervision</li> <li>With NDVI, LIVE pixels are often learned with high confidence even with 5–6 examples</li> </ul> <hr/> <p><br/></p> <h2 id="experiment-2-cross-year-generalization-train-on-2014--test-on-2020"><strong>Experiment 2: Cross-Year Generalization (Train on 2014 → Test on 2020)</strong></h2> <p>I trained a model on all 2014-labeled pixels (using 5-band input with NDVI) and tested it directly on labeled pixels from 2020 — no retraining or adaptation.</p> <p><strong>Results:</strong></p> <ul> <li><strong>Overall Accuracy</strong>: <strong>78.4%</strong></li> <li><strong>LIVE</strong> class had high precision and recall</li> <li><strong>BARE</strong> was consistently predicted correctly (recall = 1.0), though with some over-prediction</li> <li><strong>DEAD</strong> remained harder to capture (recall = 0.53)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Classification Report:
              precision    recall  f1-score
    BARE         0.571     1.000     0.727
    DEAD         0.818     0.529     0.643
    LIVE         0.885     0.885     0.885
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/confusion_2014to2020.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/confusion_2014to2020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="implications"><strong>Implications</strong></h2> <ul> <li><strong>NDVI improves generalization</strong> by providing a vegetation-specific signal that remains valid across years.</li> <li>The model is most confident on <strong>LIVE</strong> pixels, suggesting that greenness + NDVI are strong predictors.</li> <li><strong>DEAD</strong> and <strong>BARE</strong> are more difficult to distinguish — these likely require: <ul> <li>More training samples</li> <li>Spatial or temporal context (e.g., adjacent pixels, change over time)</li> </ul> </li> </ul> <hr/> <p><br/></p> <h2 id="conclusions"><strong>Conclusions</strong></h2> <ul> <li>Based on this experiment, with as few as <strong>10–13 labeled pixels per class</strong>, we can reach <strong>&gt;85% accuracy</strong> using NDVI.</li> <li>Training on one year and applying to another is feasible if the data is normalized.</li> <li>NDVI should be included as a feature — it boosts performance significantly and reduces label burden.</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Increase label count across years</li> <li>Test reverse generalization: 2020 → 2014</li> <li>Predict full raster maps using trained models</li> <li>Potentially Add 3×3 or 5×5 patch-based context around each pixel</li> </ul>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Detecting Tree Mortality from Aerial Imagery</title><link href="https://deveshparagiri.github.io/blog/2025/research-logs/" rel="alternate" type="text/html" title="Detecting Tree Mortality from Aerial Imagery"/><published>2025-05-18T20:00:00+00:00</published><updated>2025-05-18T20:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/research-logs</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/research-logs/"><![CDATA[<hr/> <h2 id="log-1-deepforest-approach"><strong>Log 1: DeepForest Approach</strong></h2> <p><br/></p> <h3 id="research-question"><strong>Research Question</strong></h3> <p>How can we reliably detect <strong>tree mortality</strong> across time using <strong>aerial imagery</strong>? Can <strong>pretrained object detection models</strong>, like <strong>DeepForest</strong>, give us useful indicators of <strong>tree health or death</strong>?</p> <hr/> <p><br/></p> <h3 id="phase-1-approach--using-deepforest-for-crown-detection"><strong>Phase 1 Approach — Using DeepForest for Crown Detection</strong></h3> <p>DeepForest is a state-of-the-art deep learning model trained on RGB imagery to detect <strong>individual tree crowns</strong>. It outputs <strong>bounding boxes</strong> around tree-like objects, which initially seemed promising for analyzing:</p> <ul> <li><strong>Tree count changes over time</strong></li> <li><strong>Tree canopy shrinkage</strong></li> <li>Mortality via <strong>absence or degradation</strong> of detected crowns</li> </ul> <hr/> <p><br/></p> <h3 id="thought-process"><strong>Thought Process</strong></h3> <p>Use a pretrained model to:</p> <ul> <li>Quickly extract structured detections</li> <li>Use <strong>bounding box count or size</strong> as a proxy for forest density</li> <li>Detect <strong>tree loss trends over time</strong> without training from scratch</li> </ul> <hr/> <p><br/></p> <h3 id="methodology"><strong>Methodology</strong></h3> <ul> <li><strong>Imagery Source:</strong> NAIP (2014–2022), 512×512 RGB tiles, resolution: 1.0m → 0.6m</li> <li><strong>Tool:</strong> <a href="https://github.com/weecology/DeepForest"><code class="language-plaintext highlighter-rouge">DeepForest</code></a></li> <li><strong>Workflow:</strong> <ul> <li>Load NAIP tile</li> <li>Predict bounding boxes using pretrained model</li> <li>Compare box count and coverage across years</li> </ul> </li> </ul> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepforest</span> <span class="kn">import</span> <span class="n">main</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">main</span><span class="p">.</span><span class="nf">deepforest</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">use_release</span><span class="p">()</span>
<span class="n">boxes</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_image</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">naip_tile.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <hr/> <p><br/></p> <h3 id="sample-outputs"><strong>Sample Outputs</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log1.png" sizes="95vw"/> <img src="/assets/img/research_log/log1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepForest detection results before parameter tuning</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log1-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log1-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepForest detection results after parameter tuning</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="challenges-encountered"><strong>Challenges Encountered</strong></h3> <p><br/></p> <h5 id="1-spatial-resolution-mismatch">1. <strong>Spatial Resolution Mismatch</strong></h5> <ul> <li>DeepForest was trained on ~0.1m resolution. NAIP tiles at 1.0m/0.6m were <strong>too coarse</strong>.</li> <li>Crowns were <strong>blurry</strong>, often <strong>undetected or merged</strong> into clusters.</li> </ul> <h5 id="2-bounding-boxes--tree-area">2. <strong>Bounding Boxes ≠ Tree Area</strong></h5> <ul> <li>Boxes were <strong>not calibrated</strong> to actual canopy area.</li> <li>Detection size and count varied erratically due to visual artifacts.</li> </ul> <h5 id="3-noisy-temporal-consistency">3. <strong>Noisy Temporal Consistency</strong></h5> <ul> <li>Detections fluctuated due to shadows, season, sun angle — <strong>not ecological change</strong>.</li> <li>Trees “disappeared” or “reappeared” randomly between years.</li> </ul> <hr/> <p><br/></p> <h3 id="summary"><strong>Summary</strong></h3> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Visual Interpretability</td> <td>❌ Low</td> </tr> <tr> <td>Spatial Precision</td> <td>❌ Weak</td> </tr> <tr> <td>Temporal Consistency</td> <td>❌ Unusable</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h3 id="key-takeaways"><strong>Key Takeaways</strong></h3> <ul> <li>Pretrained models are <strong>domain-constrained</strong></li> <li>Tree detection ≠ mortality inference</li> <li><strong>RGB-only features are too volatile</strong> over time</li> </ul> <hr/> <p><br/></p> <h3 id="transition-to-next-phase"><strong>Transition to Next Phase</strong></h3> <p>We needed a strategy focused on <strong>semantic labeling</strong> (LIVE / DEAD / BARE), not detection. This led to the <strong>5x5 patch-based classification</strong> approach — covered next in <strong>Log 2</strong>.</p> <hr/> <p><br/></p> <h2 id="log-2-patch-based-classification"><strong>Log 2: Patch-Based Classification</strong></h2> <p><br/></p> <h3 id="research-goal"><strong>Research Goal</strong></h3> <p>Label small regions of aerial imagery based on <strong>ecological intuition</strong>, using spatial patches instead of pixel-level or bounding box classification.</p> <hr/> <p><br/></p> <h3 id="phase-2-approach--55-patch-labeling-using-human-intuition"><strong>Phase 2 Approach — 5×5 Patch Labeling Using Human Intuition</strong></h3> <p>Each 512×512 tile was divided into a 5×5 grid (i.e. ~25×25 pixel patches). Each patch was visually labeled based on <strong>dominant appearance</strong>: LIVE, DEAD, or BARE.</p> <hr/> <p><br/></p> <h3 id="thought-process-1"><strong>Thought Process</strong></h3> <ul> <li>Smoother than pixel classification</li> <li>Easier than labeling full tiles</li> <li>Provides enough context for human labeling (e.g., sparse vs dense canopy)</li> </ul> <hr/> <p><br/></p> <h3 id="methodology-1"><strong>Methodology</strong></h3> <h5 id="1-patch-generation">1. <strong>Patch Generation</strong></h5> <ul> <li>Each tile → ~100+ 5×5 patches</li> <li>Recorded in <code class="language-plaintext highlighter-rouge">valid_patches.csv</code>: <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>r329_c58_y2020.png, 329, 58, 2020, , possibly BARE or DEAD
</code></pre></div> </div> </li> </ul> <h5 id="2-visual-labeling">2. <strong>Visual Labeling</strong></h5> <ul> <li>No NDVI or thresholding</li> <li>Labels were assigned via: <ul> <li>Manual inspection across years</li> <li>Texture, color, and shape</li> <li>“Hint” labels updated during review</li> </ul> </li> </ul> <h5 id="3-model-training">3. <strong>Model Training</strong></h5> <ul> <li>Feature extraction: <ul> <li>RGB stats (mean, std)</li> <li>Optional raw pixel flattening</li> </ul> </li> <li>Classifier: <ul> <li>Random Forest (baseline)</li> <li>Small MLP (later)</li> </ul> </li> </ul> <hr/> <p><br/></p> <h3 id="sample-outputs-1"><strong>Sample Outputs</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-1.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Sample patch labeling</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Area of Interest</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-3.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Dead Tree Area Over Time</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-4.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Transition Matrices for 5x5 Patch Classification</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="challenges-encountered-1"><strong>Challenges Encountered</strong></h3> <p><br/></p> <h5 id="1-labeling-conflicts-majority-dilemma">1. <strong>Labeling Conflicts (Majority Dilemma)</strong></h5> <ul> <li>Mixed-content patches (e.g., half LIVE, half DEAD)</li> <li>Label assignment was subjective → high variance</li> </ul> <h5 id="2-spectral-inconsistency">2. <strong>Spectral Inconsistency</strong></h5> <ul> <li>Same class in different years looked different (due to sensors, lighting)</li> <li>Model trained on one year <strong>couldn’t generalize</strong> to another</li> </ul> <h5 id="3-spatial-drift">3. <strong>Spatial Drift</strong></h5> <ul> <li>Misalignments between years → same patch ID pointed to <strong>different physical areas</strong></li> <li>Resolution changes (1m vs 0.6m) broke equivalence</li> </ul> <hr/> <p><br/></p> <h3 id="summary-1"><strong>Summary</strong></h3> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>In-year Accuracy</td> <td>~65–70%</td> </tr> <tr> <td>Cross-year Generalization</td> <td>❌ Failed (&lt;50%)</td> </tr> <tr> <td>Label Noise</td> <td>High</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h3 id="key-takeaways-1"><strong>Key Takeaways</strong></h3> <ul> <li><strong>Visual labeling ≠ repeatable</strong> at scale</li> <li>Patches blur meaningful distinctions</li> <li>Spectral models must <strong>anchor on spatial precision</strong></li> <li>RGB features alone cannot reliably detect long-term changes</li> </ul> <hr/> <p><br/></p> <h3 id="transition-to-next-phase-1"><strong>Transition to Next Phase</strong></h3> <p>To resolve both spectral and spatial instability, we transitioned to <strong>pixel-level temporal modeling</strong> — tracking <strong>individual pixels across all years</strong>. That’s the focus of <strong>Log 3</strong>.</p> <hr/> <p><br/></p> <h2 id="log-3-single-pixel-temporal-classification"><strong>Log 3: Single Pixel Temporal Classification</strong></h2> <p><br/></p> <h3 id="research-goal-1"><strong>Research Goal</strong></h3> <p>Track <strong>individual pixels</strong> over time and use their <strong>temporal NDVI trajectories</strong> to classify them into ecological categories (LIVE, DEAD, BARE).</p> <hr/> <p><br/></p> <h3 id="phase-3-approach--per-pixel-temporal-ndvi-classifier"><strong>Phase 3 Approach — Per-Pixel Temporal NDVI Classifier</strong></h3> <p>We moved away from patches and bounding boxes entirely. Each pixel became its own data point, tracked across all available years.</p> <hr/> <p><br/></p> <h3 id="thought-process-2"><strong>Thought Process</strong></h3> <ul> <li>Control exact <strong>spatial location</strong></li> <li>Use <strong>temporal signal</strong> as primary feature (e.g., NDVI over time)</li> <li>Detect transitions like LIVE → DEAD or DEAD → BARE</li> <li>Label using <strong>human-in-the-loop hints</strong> supported by NDVI</li> </ul> <hr/> <p><br/></p> <h3 id="methodology-2"><strong>Methodology</strong></h3> <h5 id="1-pixel-extraction">1. <strong>Pixel Extraction</strong></h5> <ul> <li>Spatially aligned tiles across years (2014–2022)</li> <li>Pixels indexed by <code class="language-plaintext highlighter-rouge">row</code>, <code class="language-plaintext highlighter-rouge">col</code></li> <li> <p>Each pixel assigned time-series NDVI values:</p> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>filename,row,col,year,ndvi,label_hint
r327_c59_y2020.png,327,59,2020,0.162,"possibly BARE or DEAD"

</code></pre></div> </div> </li> </ul> <h5 id="2-labeling-strategy">2. <strong>Labeling Strategy</strong></h5> <ul> <li>Combined: <ul> <li>Visual inspection across years</li> <li>NDVI pattern (e.g., drop then flatten)</li> <li>Human-annotated hints like “likely DEAD”</li> </ul> </li> </ul> <h5 id="3-model-input">3. <strong>Model Input</strong></h5> <ul> <li>Each training sample: NDVI vector across years <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>[0.58 (2014), 0.56 (2016), 0.44 (2018), 0.23 (2020), 0.22 (2022)]
→ label: DEAD
</code></pre></div> </div> </li> </ul> <hr/> <p><br/></p> <h3 id="sample-outputs-2"><strong>Sample Outputs</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/raw_buffer_2014.png" sizes="95vw"/> <img src="/assets/img/research_log/raw_buffer_2014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Raw Buffer 2014</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/matched_buffer_2014.png" sizes="95vw"/> <img src="/assets/img/research_log/matched_buffer_2014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Spectrally Matched Buffer 2014</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log3-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log3-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Vegetation Reflectance Drift</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log3-1.png" sizes="95vw"/> <img src="/assets/img/research_log/log3-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Single Pixel Time Series</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="key-strengths"><strong>Key Strengths</strong></h3> <ul> <li>True <strong>temporal stability</strong> — fixed pixel over time</li> <li>Model can <strong>learn transitions</strong>, not just snapshot classes</li> <li>Eliminates resolution-induced mapping drift</li> </ul> <hr/> <p><br/></p> <h3 id="experimental-insights"><strong>Experimental Insights</strong></h3> <ul> <li>Temporal modeling removes much spatial noise</li> <li>NDVI change patterns (drop + flatten) correlate well with true mortality</li> <li>Model interpretability improves (you can “see” what happened)</li> </ul> <hr/> <p><br/></p> <h3 id="current-status"><strong>Current Status</strong></h3> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Temporal NDVI consistency</td> <td>✅ Strong</td> </tr> <tr> <td>Human label quality</td> <td>✅ High confidence</td> </tr> <tr> <td>Class balance</td> <td>⚠️ Still tuning</td> </tr> <tr> <td>Next step</td> <td>Expand labeled samples &amp; train temporal classifier</td> </tr> </tbody> </table> <hr/>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">AI Agent vs Workflow</title><link href="https://deveshparagiri.github.io/blog/2025/agent-vs-workflow-copy/" rel="alternate" type="text/html" title="AI Agent vs Workflow"/><published>2025-03-26T20:00:00+00:00</published><updated>2025-03-26T20:00:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2025/agent-vs-workflow%20copy</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2025/agent-vs-workflow-copy/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <p><br/></p> <p>AI agents are all the hype. 2025 is touted to be the year of agents. Some say they’ll take over all our jobs; others call them an over-engineered gimmick. Regardless of where you stand, one thing is clear: <strong>agents are here to stay</strong>, and they’re poised to reshape how developers build and interact with software.</p> <p>But before you jump on the hype train, it’s worth asking – <em>what actually sets agents apart from traditional AI workflows?</em> And when should you use one over another?</p> <p>This blog breaks down the technical differences between agents and workflows – just the key details with implications for your tech stack.</p> <p><br/></p> <h2 id="agents-vs-workflows---tldr"><strong>Agents vs Workflows - TLDR</strong></h2> <p><br/></p> <table> <thead> <tr> <th><strong>Feature</strong></th> <th><strong>AI Agent</strong></th> <th><strong>AI Workflow</strong></th> </tr> </thead> <tbody> <tr> <td>Core Idea</td> <td>Autonomous Entity with goals + reasoning</td> <td>Perform a defined sequence of tasks</td> </tr> <tr> <td>State</td> <td>Has memory, state, and internal feedback loops</td> <td>Usually static or explicit state</td> </tr> <tr> <td>Flexibility</td> <td>Dynamic, pivots according to the requirement</td> <td>Predefined, changes require editing the flow</td> </tr> <tr> <td>Determinism</td> <td>Non-deterministic; same prompt ≠ same output</td> <td>Deterministic (mostly)</td> </tr> <tr> <td>Sample use-case</td> <td>Open-ended tasks, multiple steps, vague goals</td> <td>Structured, repeated, and known flows</td> </tr> </tbody> </table> <p><br/></p> <h2 id="so-what-is-an-agent"><strong>So what is an Agent?</strong></h2> <p><br/> Think of an agent as your mini-employee with a brain, a set of tasks, and access to tools.</p> <p>You give it a goal and it figures out how to get there – which tools to use, questions to ask, and maybe even ask clarifying questions. The agent has full autonomy every step of the way, including the output.</p> <p>An agent can evaluate it’s own response and choose to restart it’s task. Effectively, the agent can optimize it’s own processes.</p> <p><br/></p> <h4 id="core-behaviors"><strong>Core Behaviors:</strong></h4> <ul> <li><strong>Goal-oriented:</strong> You say “Summarize this repo and build a README,” it plans its own steps.</li> <li><strong>Tool-using:</strong> Calls APIs, scrapes data, runs subprocesses — whatever gets the job done.</li> <li><strong>Memory-aware:</strong> Keeps track of prior steps, retries intelligently.</li> <li><strong>Autonomous (ish):</strong> Can decide its own flow, within guardrails.</li> </ul> <p><br/></p> <h3 id="sample-agent-in-action"><strong>Sample Agent in Action</strong></h3> <p><br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/agent_vs_workflow/asset.webp" sizes="95vw"/> <img src="/assets/img/agent_vs_workflow/asset.webp" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">AI Agent Decision Flow Diagram. Source: <a href="https://www.anthropic.com/engineering/building-effective-agents">Anthropic</a></figcaption> </figure> </div> </div> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">agent</span> <span class="o">=</span> <span class="nc">Agent</span><span class="p">(</span>
    <span class="n">role</span><span class="o">=</span><span class="sh">"</span><span class="s">Junior Dev</span><span class="sh">"</span><span class="p">,</span>
    <span class="n">tools</span><span class="o">=</span><span class="p">[</span><span class="n">search_docs</span><span class="p">,</span> <span class="n">write_code</span><span class="p">,</span> <span class="n">run_tests</span><span class="p">,</span> <span class="n">evaluate_code</span><span class="p">,</span> <span class="n">human_input</span><span class="p">],</span>
    <span class="n">memory</span><span class="o">=</span><span class="nc">Memory</span><span class="p">(),</span>
<span class="p">)</span>

<span class="n">result</span> <span class="o">=</span> <span class="n">agent</span><span class="p">.</span><span class="nf">run</span><span class="p">(</span><span class="sh">"</span><span class="s">Create Quarto Docs for this repo.</span><span class="sh">"</span><span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h2 id="so-what-is-a-workflow"><strong>So what is a Workflow?</strong></h2> <p><br/></p> <p>A <strong>workflow</strong> is more like a flowchart. You lay out the steps: “Do A, then B, then C.” It’s deterministic, composable, and rock-solid for anything structured.</p> <p>Think CI/CD pipelines, document processing chains, or RAG pipelines.</p> <p><br/></p> <h4 id="core-behaviors-1"><strong>Core Behaviors:</strong></h4> <ul> <li><strong>Step-by-step logic:</strong> The flow is explicit and predictable.</li> <li><strong>Composable:</strong> You can chain blocks together easily.</li> <li><strong>Debuggable:</strong> You know exactly where something breaks.</li> <li><strong>Efficient:</strong> Less overhead, faster runtime.</li> </ul> <p><br/></p> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">chain</span> <span class="o">=</span> <span class="p">(</span>
    <span class="p">{</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="nc">RunnablePassthrough</span><span class="p">()}</span>
    <span class="o">|</span> <span class="p">{</span><span class="sh">"</span><span class="s">docs</span><span class="sh">"</span><span class="p">:</span> <span class="n">retriever</span><span class="p">,</span> <span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">:</span> <span class="k">lambda</span> <span class="n">x</span><span class="p">:</span> <span class="n">x</span><span class="p">[</span><span class="sh">"</span><span class="s">query</span><span class="sh">"</span><span class="p">]}</span>
    <span class="o">|</span> <span class="p">{</span><span class="sh">"</span><span class="s">answer</span><span class="sh">"</span><span class="p">:</span> <span class="n">rag_chain</span><span class="p">}</span>
<span class="p">)</span>
</code></pre></div></div> <p><br/></p> <h2 id="what-about-a-hybrid"><strong>What about a hybrid?</strong></h2> <p><br/></p> <p>Some of the best systems mix both. You could use a workflow to spawn agents, or let agents call reliable AI workflows. For example:</p> <ul> <li>AI agent for making a landing page calls several workflows - outline generator, code generator, deployment workflow</li> </ul> <p>You get the flexibility of agents without compromising on the reliability of workflows.</p> <p><br/></p> <h2 id="conclusion"><strong>Conclusion</strong></h2> <p><br/></p> <p>Agents are <em>not</em> here to replace workflows. They’re here to unlock new kinds of problems we couldn’t automate before.</p> <p>Use <strong>agents</strong> when you want flexible, autonomous decision-making.</p> <p>Use <strong>workflows</strong> when you want reliable, fast, auditable pipelines.</p> <p>Sometimes you need brains. Sometimes you need structure. And sometimes — you need both.</p> <p><br/></p> <h2 id="helpful-links-and-resources"><strong>Helpful Links and Resources</strong></h2> <p><br/></p> <ul> <li><a href="https://www.youtube.com/watch?v=tx5OapbK-8A">Build your first agent</a></li> <li><a href="https://www.youtube.com/watch?v=4nZl32FwU-o">Multi-agent architectures</a></li> <li><a href="https://www.langflow.org/">Low-code agent builder</a></li> <li><a href="https://github.com/e2b-dev/awesome-ai-agents">AI Agent Repository</a></li> </ul>]]></content><author><name></name></author><category term="code"/><category term="ai"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Deploying a node.JS app to AWS EC2</title><link href="https://deveshparagiri.github.io/blog/2024/deploy-nodejs/" rel="alternate" type="text/html" title="Deploying a node.JS app to AWS EC2"/><published>2024-11-20T15:59:00+00:00</published><updated>2024-11-20T15:59:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2024/deploy-nodejs</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2024/deploy-nodejs/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <p>You’ve successfully built your application, and now you’re looking forward to deploying it! This blog covers the basics of <strong>AWS EC2</strong>, setup, and deployment. <br/> <br/></p> <h2 id="setup-an-aws-ec2-instance"><strong>Setup an AWS EC2 Instance</strong></h2> <p><br/></p> <h3 id="what-even-is-ec2"><strong>What even is EC2?</strong></h3> <p><strong>EC2</strong> stands for <strong>Elastic Cloud Compute</strong> and is a virtual machine where you can deploy your applications. What’s so cool about it? It allows for easy <strong>scaling</strong> to handle more requests coming into your application when the need arises, making it easy to handle varying demand. It’s also quite versatile, making it a great choice for developers.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/balancer.png" sizes="95vw"/> <img src="/assets/img/blog1/balancer.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h3 id="why-are-we-using-it"><strong>Why are we using it?</strong></h3> <p>It’s simple, ease of use, and quick setup makes it a great first choice for those getting started with development. Above all, it’s cheap because of <strong>“on-demand” scaling</strong> helping to only pull out the big guns when absolutely required. Alternatively, we could also use an <strong>Azure VM</strong> or the <strong>Google Cloud Engine (GCE)</strong> to deploy our app, which I will cover in later blogs.</p> <p><br/></p> <h3 id="getting-started"><strong>Getting Started</strong></h3> <p>Let’s set up the AWS instance and connect to it via <strong>SSH</strong>.</p> <h4 id="1-login-to-your-aws-account-root-user">1. Login to your AWS account (Root User)</h4> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/awslogin.png" sizes="95vw"/> <img src="/assets/img/blog1/awslogin.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h4 id="2-launching-an-instance">2. Launching an instance</h4> <p>Under the <strong>EC2</strong>, choose <strong>launch instance</strong>. For this application, we will use <strong>AWS Linux</strong> as our OS Image due to its tight integration with AWS and ease of use. Alternatively, you could also use <strong>Ubuntu</strong> but will have to make some changes (<code class="language-plaintext highlighter-rouge">apt-get</code> vs. <code class="language-plaintext highlighter-rouge">yum</code>) for the next steps during installing dependencies.</p> <p>Choose <strong>t2.micro</strong> for instance type – simplest instance, and is very cost effective. If your application is quite large and compute intensive, shift to <strong>t2.medium</strong> for ensuring enough memory capacity.</p> <p>We create a new <strong>key pair</strong>, which will enable us to connect to our instance through <strong>SSH</strong> and get started with setting up. A <code class="language-plaintext highlighter-rouge">.pem</code> file will be downloaded to your local machine.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/awskeypair.png" sizes="95vw"/> <img src="/assets/img/blog1/awskeypair.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>We allow all traffic, so we can access the instance from anywhere. We will be adding some inbound rules later on to make our port accessible to the public DNS.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/blog1/awsnetwork.png" sizes="95vw"/> <img src="/assets/img/blog1/awsnetwork.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p><br/></p> <h4 id="3-connecting-to-instance-ssh">3. Connecting to Instance (SSH)</h4> <p>To connect, you must first download the <code class="language-plaintext highlighter-rouge">.pem</code> file from when you set up the EC2 instance. Execute the below commands.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">chmod </span>400 &lt;path/to/security-key.pem&gt; <span class="c">#read permissions</span>

ssh <span class="nt">-i</span> &lt;path/to/security-key.pem&gt; ec2-user@ec2-&lt;Ipv4 Address&gt;.compute.amazonaws.com
</code></pre></div></div> <p><br/></p> <h2 id="installing-dependencies"><strong>Installing Dependencies</strong></h2> <p>The dependencies we will require include:</p> <ul> <li>node</li> <li>git</li> <li>nginx</li> <li>pm2</li> </ul> <p>Ensure all packages are up-to-date</p> <p>Ensure all packages are up-to-date</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>yum upgrade <span class="o">&amp;&amp;</span> <span class="nb">sudo </span>yum update
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Installing Node</span>

curl <span class="nt">-o-</span> https://raw.githubusercontent.com/nvm-sh/nvm/v0.34.0/install.sh | bash
<span class="nb">.</span> ~/.nvm/nvm.sh
nvm <span class="nb">install </span>node

node <span class="nt">-v</span>
<span class="c"># v22.9.0</span>
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Installing and Setting up Git</span>

yum <span class="nb">install </span>git <span class="nt">-y</span>

ssh-keygen <span class="nt">-t</span> ed25519 <span class="nt">-C</span> <span class="s2">"your email"</span>
<span class="nb">eval</span> <span class="s2">"</span><span class="si">$(</span>ssh-agent <span class="nt">-s</span><span class="si">)</span><span class="s2">"</span>
ssh-add ~/.ssh/id_ed25519

<span class="nb">cat</span> ~/.ssh/id_ed25519.pub
ssh-keyscan github.com <span class="o">&gt;&gt;</span> ~/.ssh/known_hosts
ssh <span class="nt">-T</span> git@github.com

git clone &lt;Your Repository&gt;
</code></pre></div></div> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c"># Installing pm2 and nginx</span>

npm <span class="nb">install</span> <span class="nt">-g</span> pm2

<span class="nb">sudo </span>yum <span class="nb">install </span>nginx <span class="nt">-y</span>
<span class="nb">sudo </span>systemctl start nginx
<span class="nb">sudo </span>systemctl status nginx
</code></pre></div></div> <p><br/></p> <h2 id="deployment"><strong>Deployment</strong></h2> <p>We will use nginx as a reverse proxy forwarding all inbound calls from port 80 to our application. For now, we will only focus on HTTP. Now, under <code class="language-plaintext highlighter-rouge">/etc/nginx/sites-available/</code> modify the <code class="language-plaintext highlighter-rouge">default</code> config.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>server <span class="o">{</span>
    listen 80<span class="p">;</span>
    server_name your-server-ip-or-domain<span class="p">;</span>

    location / <span class="o">{</span>
        proxy_pass http://127.0.0.1:3000<span class="p">;</span> <span class="c"># Replace with your app's port</span>
        proxy_http_version 1.1<span class="p">;</span>
        proxy_set_header Upgrade <span class="nv">$http_upgrade</span><span class="p">;</span>
        proxy_set_header Connection <span class="s1">'upgrade'</span><span class="p">;</span>
        proxy_set_header Host <span class="nv">$host</span><span class="p">;</span>
        proxy_cache_bypass <span class="nv">$http_upgrade</span><span class="p">;</span>
    <span class="o">}</span>
<span class="o">}</span>
</code></pre></div></div> <p>We test the nginx config and restart to apply new changes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="nb">sudo </span>nginx <span class="nt">-t</span>
<span class="nb">sudo </span>systemctl restart nginx
</code></pre></div></div> <p>To learn more about nginx, refer this <a href="https://nginx.org/en/docs/beginners_guide.html">guide</a>.</p> <p>Finally, we use pm2 to ensure that the application runs continuously and restarts automatically if it crashes.</p> <div class="language-bash highlighter-rouge"><div class="highlight"><pre class="highlight"><code>pm2 start app.js <span class="nt">--name</span> <span class="s2">"my-app"</span> <span class="c"># Replace `app.js` with your main file name</span>
pm2 save

pm2 list <span class="c"># Shows if the app is running</span>
pm2 log <span class="c"># Check deployment log</span>

pm2 save <span class="c"># Save process list to enable auto-restart on server reboot</span>
pm2 startup systemd <span class="c"># Generate startup script for reboot</span>
</code></pre></div></div> <p>You should now be able to access the application through the <code class="language-plaintext highlighter-rouge">Ipv4</code> DNS of your server!</p>]]></content><author><name></name></author><category term="code"/><category term="cloud"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Pycon India 2023 — trip report</title><link href="https://deveshparagiri.github.io/blog/2023/pycon-india-2023trip-report/" rel="alternate" type="text/html" title="Pycon India 2023 — trip report"/><published>2023-10-08T05:53:54+00:00</published><updated>2023-10-08T05:53:54+00:00</updated><id>https://deveshparagiri.github.io/blog/2023/pycon-india-2023trip-report</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2023/pycon-india-2023trip-report/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Secure AI Bot for Private Data</title><link href="https://deveshparagiri.github.io/blog/2023/secure-ai-bot-for-private-data/" rel="alternate" type="text/html" title="Secure AI Bot for Private Data"/><published>2023-09-19T12:29:15+00:00</published><updated>2023-09-19T12:29:15+00:00</updated><id>https://deveshparagiri.github.io/blog/2023/secure-ai-bot-for-private-data</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2023/secure-ai-bot-for-private-data/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Deploy a Flask Web App to AWS Beanstalk (CI/CD with Git)</title><link href="https://deveshparagiri.github.io/blog/2023/deploy-a-flask-web-app-to-aws-beanstalk-cicd-with-git/" rel="alternate" type="text/html" title="Deploy a Flask Web App to AWS Beanstalk (CI/CD with Git)"/><published>2023-08-25T12:35:04+00:00</published><updated>2023-08-25T12:35:04+00:00</updated><id>https://deveshparagiri.github.io/blog/2023/deploy-a-flask-web-app-to-aws-beanstalk-cicd-with-git</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2023/deploy-a-flask-web-app-to-aws-beanstalk-cicd-with-git/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Hosting a Website Using AWS EC2</title><link href="https://deveshparagiri.github.io/blog/2023/hosting-a-website-using-aws-ec2/" rel="alternate" type="text/html" title="Hosting a Website Using AWS EC2"/><published>2023-08-08T05:53:40+00:00</published><updated>2023-08-08T05:53:40+00:00</updated><id>https://deveshparagiri.github.io/blog/2023/hosting-a-website-using-aws-ec2</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2023/hosting-a-website-using-aws-ec2/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">OpenAI Whisper API for Transcribing Any YouTube Video</title><link href="https://deveshparagiri.github.io/blog/2023/openai-whisper-api-for-transcribing-any-youtube-video/" rel="alternate" type="text/html" title="OpenAI Whisper API for Transcribing Any YouTube Video"/><published>2023-04-10T09:24:00+00:00</published><updated>2023-04-10T09:24:00+00:00</updated><id>https://deveshparagiri.github.io/blog/2023/openai-whisper-api-for-transcribing-any-youtube-video</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2023/openai-whisper-api-for-transcribing-any-youtube-video/"><![CDATA[]]></content><author><name></name></author></entry><entry><title type="html">Getting started with multivariate linear regression</title><link href="https://deveshparagiri.github.io/blog/2021/getting-started-with-multivariate-linear-regression/" rel="alternate" type="text/html" title="Getting started with multivariate linear regression"/><published>2021-08-19T16:22:50+00:00</published><updated>2021-08-19T16:22:50+00:00</updated><id>https://deveshparagiri.github.io/blog/2021/getting-started-with-multivariate-linear-regression</id><content type="html" xml:base="https://deveshparagiri.github.io/blog/2021/getting-started-with-multivariate-linear-regression/"><![CDATA[]]></content><author><name></name></author></entry></feed>