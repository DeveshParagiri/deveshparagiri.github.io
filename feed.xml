<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.4.1">Jekyll</generator><link href="https://deveshparagiri.com/feed.xml" rel="self" type="application/atom+xml"/><link href="https://deveshparagiri.com/" rel="alternate" type="text/html" hreflang="en"/><updated>2026-02-14T00:04:53+00:00</updated><id>https://deveshparagiri.com/feed.xml</id><title type="html">blank</title><subtitle>Dev Paragiri&apos;s personal website. </subtitle><entry><title type="html">The Case for Predictive Intelligence</title><link href="https://deveshparagiri.com/blog/2025/entropy-00/" rel="alternate" type="text/html" title="The Case for Predictive Intelligence"/><published>2025-11-20T15:59:00+00:00</published><updated>2025-11-20T15:59:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/entropy-00</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/entropy-00/"><![CDATA[<p><em>Why we are building Entropy</em></p> <hr/> <p>Every major strategic decision is a bet on human behavior.</p> <p>Whether you are restructuring a corporation, launching a disruptive product, or shaping public policy, you are effectively wagering on how thousands of independent actors will react to a new reality. The stakes are absolute, but the tools we use to place these bets are archaic.</p> <p>We rely on surveys, which capture what people <em>say</em> they will do, not what they actually do. We rely on focus groups, which reduce complex populations to a handful of voices in a conference room. We rely on historical data, forcing us to drive into the future while looking in the rearview mirror.</p> <p>The fundamental problem with traditional research is the observer effect: <strong>You cannot test the future without creating it.</strong></p> <p>To measure employee sentiment regarding a reorganization, you must ask them about it—triggering the very anxiety you sought to measure. To test a pricing strategy, you must expose it to the market—tipping your hand to competitors. To gauge voter reaction to a controversial policy, you must announce it—and live with the consequences if you guessed wrong.</p> <p>We are building Entropy to break this paradox.</p> <hr/> <h3 id="simulation-as-strategy"><strong>Simulation as Strategy</strong></h3> <hr/> <p>Entropy is a simulation engine that allows leaders to test the future in a low-stakes environment before committing to it in a high-stakes reality.</p> <p>We do not ask people what they would do. We create synthetic populations—statistically grounded in real-world demographics, psychographics, and network structures—and we let them act.</p> <p>This is an ontological shift in how we understand the world. We don’t ask clouds if they intend to rain; we model atmospheric dynamics. We don’t ask bridges if they intend to collapse; we model structural physics. Yet, when it comes to the most complex system of all—human behavior—we are still stuck asking for opinions.</p> <p>Entropy brings the rigor of computational modeling to social science.</p> <ol> <li> <p><strong>Grounded Fidelity:</strong> We generate agents based on deep research, not generic templates. A simulated German surgeon possesses the specific professional attributes, institutional rank, and peer incentives that drive real surgical behavior. A simulated swing-state voter possesses the specific media diet and economic anxieties of their district.</p> </li> <li> <p><strong>Dynamic Evolution:</strong> Surveys capture a static snapshot. Simulation captures motion. We model how information travels through a network, how early adopters influence the late majority, and how sentiment decays or hardens over time.</p> </li> <li> <p><strong>Convergence:</strong> As we increase the number of agents, variance decreases. By simulating at scale—ten thousand agents, one hundred thousand agents—individual noise cancels out, and population-level truths emerge.</p> </li> </ol> <hr/> <h3 id="the-end-of-guessing"><strong>The End of Guessing</strong></h3> <hr/> <p>The implications of this technology are profound.</p> <p>An organization can simulate a restructuring plan to identify specific pockets of retention risk before a single memo is sent. A product team can model the churn impact of a price increase across different user cohorts without risking a public backlash. A campaign can test the second-order effects of a policy announcement, measuring not just immediate reaction, but how the narrative evolves over weeks of social transmission.</p> <p>This provides the one thing traditional research cannot: <strong>The ability to be wrong in private.</strong></p> <p>You can run the simulation a thousand times. You can fail nine hundred times. You can adjust the variables, refine the messaging, and iterate on the strategy until the model converges on success. And only then do you execute in the real world.</p> <p>We model supply chains before we build factories. We model aerodynamics before we fly planes. It is time we modeled the human response before we make history.</p> <p>The future of strategy is not polling. It is prediction.</p> <p><em>Entropy is currently under active development at this time of writing. The following posts in this series explore the <a href="/blog/2025/entropy-01">technical architecture</a> enabling this vision.</em></p>]]></content><author><name></name></author><category term="code"/><category term="ai"/><summary type="html"><![CDATA[Why we are building Entropy]]></summary></entry><entry><title type="html">Fetch-Weighted Tower Analysis of Canopy Mortality at US-MPJ</title><link href="https://deveshparagiri.com/blog/2025/fetch/" rel="alternate" type="text/html" title="Fetch-Weighted Tower Analysis of Canopy Mortality at US-MPJ"/><published>2025-07-14T16:00:00+00:00</published><updated>2025-07-14T16:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/fetch</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/fetch/"><![CDATA[<h3 id="introduction"><strong>Introduction</strong></h3> <hr/> <ul> <li><strong>Goal</strong>: Understand vegetation change and mortality around the US-MPJ flux tower</li> <li><strong>Motivation</strong>: Prior studies used a simple radial extraction (e.g. 200 m radius) to link predicted tree class maps to tower fluxes like GPP</li> <li><strong>Limitation</strong>: This radial assumption ignores <strong>atmospheric fetch directionality and influence distribution</strong></li> </ul> <hr/> <h3 id="limitations-of-simple-radial-analysis"><strong>Limitations of Simple Radial Analysis</strong></h3> <hr/> <ul> <li>The original 200 m circular masks assume isotropic contribution from surrounding pixels</li> <li>GPP (Gross Primary Productivity) measured by eddy covariance towers is influenced by a <strong>non-uniform footprint</strong></li> <li>This led us to revisit the approach using <strong>physical footprint modeling</strong></li> </ul> <hr/> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/predictions.png" sizes="95vw"/> <img src="/assets/img/model_eval/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – 200m Radial Class Trends</figcaption> </figure> </div> </div> <hr/> <h3 id="fetch-weighted-modeling-approach"><strong>Fetch-Weighted Modeling Approach</strong></h3> <hr/> <p>Our approach utilizes half-hourly Ameriflux tower meteorology data, including wind speed (WS), wind direction (WD), friction velocity (USTAR), measurement height (zm), roughness length (z0), and canopy height (h).</p> <p>For each year of analysis, we first extract all valid half-hourly meteorological records from the tower data. For each individual record, we then compute a 2D footprint using a custom Gaussian spread kernel that accounts for atmospheric dispersion patterns.</p> <p>Finally, we accumulate and normalize these individual footprints across all valid records to produce an annual influence map \(W\) that represents the spatial distribution of tower measurement sensitivity.</p> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/plot.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Footprint Weight Density</figcaption> </figure> </div> </div> <hr/> <h3 id="fetch-weighted-class-composition-trends"><strong>Fetch-Weighted Class Composition Trends</strong></h3> <hr/> <p>For each year, we projected the annual footprint weight map \(W\) onto the corresponding prediction raster (classes: LIVE = 0, DEAD = 1, BARE = 2). Each \(W\) was interpolated to match the raster resolution.</p> <p>We then computed fetch-weighted class fractions using:</p> <hr/> \[\text{LIVE}_{\text{weighted}} = \sum_{i}\sum_{j:\,P_{i,j}=0} W_{i,j}\] \[\text{DEAD}_{\text{weighted}} = \sum_{i}\sum_{j:\,P_{i,j}=1} W_{i,j}\] \[\text{BARE}_{\text{weighted}} = \sum_{i}\sum_{j:\,P_{i,j}=2} W_{i,j}\] <hr/> <p>Since \(W\) is normalized \((Σ W = 1)\), these directly represent the percent contribution of each class to the tower’s GPP footprint.</p> <p>Confidence intervals were derived using model precision:</p> <ul> <li>LIVE: 95%</li> <li>DEAD: 62%</li> <li>BARE: 87%</li> </ul> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/predictions.png" sizes="95vw"/> <img src="/assets/img/model_eval/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/fetch_plot.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/fetch_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>The fetch-weighted analysis reveals a more pronounced decline in DEAD class composition and demonstrates greater stability in BARE class trends compared to the simple radial approach.</p> <hr/> <h3 id="understanding-activation-weight-distribution"><strong>Understanding Activation: Weight Distribution</strong></h3> <hr/> <p>The distribution of footprint weights \(W\) is extremely skewed. While the full raster may span up to 1 km², only a small fraction of pixels meaningfully influence the tower signal. Most pixels have weights near zero.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/distr.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/distr.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>This confirms that fetch-based analysis is fundamentally different from circular masking. A 200 m radius includes thousands of irrelevant pixels, whereas fetch weighting highlights only the core contributing region.</p> <hr/> <h4 id="visualizing-activation-regions">Visualizing Activation Regions</h4> <ul> <li>Overlaid high-weight contours on prediction maps to reveal where influence was concentrated</li> <li>Only a small fraction of pixels within the 200 m zone carried meaningful weight</li> </ul> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/fetch_blog/contour.png" sizes="95vw"/> <img src="/assets/img/fetch_blog/contour.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <h3 id="next-steps"><strong>Next Steps</strong></h3> <hr/> <ul> <li>Expand analysis to a <strong>1 km² area</strong></li> <li>Integrate <strong>higher-frequency (HH) Ameriflux data</strong> to refine temporal precision</li> <li>Combine annual footprints with <strong>seasonal GPP partitions</strong></li> <li>Use <strong>cumulative multi-year fetch</strong> to study legacy effects</li> <li>Run <strong>per-class NDVI or canopy height</strong> trends within weighted zone</li> </ul> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Transition Analysis and Validation with AmeriFlux GPP</title><link href="https://deveshparagiri.com/blog/2025/model-eval/" rel="alternate" type="text/html" title="Transition Analysis and Validation with AmeriFlux GPP"/><published>2025-06-24T16:00:00+00:00</published><updated>2025-06-24T16:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/model-eval</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/model-eval/"><![CDATA[<h2 id="introduction"><strong>Introduction</strong></h2> <hr/> <p>The US-MPJ site has exhibited conflicting trends in ecosystem productivity from 2010 onward. While ED-LiDAR reconstructions indicate stable or rising productivity, both Landsat NDVI and AmeriFlux tower observations (GPP) show a marked decline. This divergence raises a key question: Is rapid, large-scale canopy mortality being missed by traditional models?</p> <p>To investigate, we developed a lightweight image-based classifier using high-resolution NAIP aerial imagery to directly detect tree mortality. We then analyzed class transitions across time (2014–2022) and validated findings against tower-based GPP observations.</p> <hr/> <h2 id="model-retraining-with-updated-labels"><strong>Model Retraining with Updated Labels</strong></h2> <hr/> <p>We curated a refined labeled dataset of 1500 high-confidence samples across three classes: <code class="language-plaintext highlighter-rouge">LIVE, DEAD, BARE</code>. Labeling combined NDVI-based filtering and visual inspection across 5 NAIP years.</p> <p>For each pixel, we extracted:</p> <ul> <li>Red, Green, Blue, NIR</li> <li>NDVI = (NIR - Red) / (NIR + Red)</li> </ul> <hr/> <h5 id="classifier-details"><strong>Classifier Details</strong></h5> <ul> <li><strong>Model</strong>: Random Forest (100 trees)</li> <li><strong>Sampling</strong>: Stratified 80/20 train-test split</li> <li><strong>Balancing</strong>: Class weights set to “balanced”</li> <li><strong>Accuracy</strong>: 0.838</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score   support
       LIVE       0.86      0.81      0.83        31
       DEAD       0.75      0.78      0.77        23
       BARE       0.90      0.95      0.93        20
</code></pre></div></div> <hr/> <h2 id="temporal-transition-analysis-20142022"><strong>Temporal Transition Analysis (2014–2022)</strong></h2> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/predictions.png" sizes="95vw"/> <img src="/assets/img/model_eval/predictions.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – Heatmap of Plausibility (Simple Score) Across Fixed Drift Combinations</figcaption> </figure> </div> </div> <p>For each pair of years, we exhaustively tested 625 spatial drift combinations, and selected the configuration minimizing biologically implausible transitions (e.g., DEAD → BARE)</p> <hr/> <table> <thead> <tr> <th>Year Pair</th> <th>Best Drift</th> <th>Notes</th> </tr> </thead> <tbody> <tr> <td>2014 → 2016</td> <td>(0, 3, 0, 4)</td> <td>Minor LIVE → DEAD, DEAD → BARE</td> </tr> <tr> <td>2016 → 2018</td> <td>(0, 3, 0, 2)</td> <td>Significant DEAD → BARE</td> </tr> <tr> <td>2018 → 2020</td> <td>(0, 0, 0, 1)</td> <td>LIVE → LIVE recovery pattern</td> </tr> <tr> <td>2020 → 2022</td> <td>(0, 1, 0, 0)</td> <td>BARE plateaued</td> </tr> </tbody> </table> <p><br/></p> <h4 id="observations"><strong>Observations</strong></h4> <ul> <li>The 2016–2018 period shows the most pronounced shift toward BARE</li> <li>Post-2020 suggests stabilization</li> </ul> <hr/> <h2 id="pixel-class-trends-over-time"><strong>Pixel Class Trends Over Time</strong></h2> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/r.png" sizes="95vw"/> <img src="/assets/img/model_eval/r.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Raster Trend</figcaption> </figure> </div> </div> <p><strong>Confidence Intervals:</strong> Based on classifier precision (LIVE: 95%, DEAD: 62%, BARE: 87%)</p> <ul> <li><strong>BARE</strong>: Steady increase throughout</li> <li><strong>DEAD</strong>: Falls after 2018, partial rebound by 2022</li> <li><strong>LIVE</strong>: Rises until 2020, then declines sharply</li> </ul> <hr/> <h2 id="cross-validation-with-ameriflux-gpp"><strong>Cross-Validation with AmeriFlux GPP</strong></h2> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/model_eval/gpp.png" sizes="95vw"/> <img src="/assets/img/model_eval/gpp.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – GPP vs Model</figcaption> </figure> </div> </div> <p>This analysis is based on AmeriFlux GPP data from the US-MPJ site.</p> <ul> <li>Spearman ρ = <strong>-0.800</strong>, p = 0.200 (n = 4)</li> <li>Moderate-to-strong negative trend between canopy loss and productivity</li> </ul> <p>Despite small sample size, the directionality supports hypothesis. BARE% rise coincides with sharp GPP fall (2016–2020)</p> <hr/> <h2 id="discussion"><strong>Discussion</strong></h2> <hr/> <ul> <li><strong>Drift correction</strong> improves temporal consistency in pixel-wise transitions</li> <li><strong>Model confidence</strong> (especially for BARE) lends weight to ecological interpretation</li> <li><strong>ED-LiDAR reconstructions</strong> likely miss rapid disturbance pulses</li> </ul> <p><strong>Limitations:</strong> Spatial resolution mismatch between tower and NAIP raster, small test set; more ground truth would improve model robustness</p> <hr/> <h2 id="conclusion"><strong>Conclusion</strong></h2> <hr/> <p>Our approach demonstrates that lightweight, image-based classifiers can reveal large-scale canopy mortality trends consistent with independent tower and satellite records. These models offer a promising supplement to traditional ecological reconstructions.</p> <p>Next steps:</p> <ul> <li>Apply SIFT for more complex transition matching</li> <li>Validate with 2022 AmeriFlux GPP</li> <li>Package the tool for broader deployment in mortality monitoring</li> </ul> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Introduction]]></summary></entry><entry><title type="html">Drift Configuration Evaluation and Transition Matrix Optimization</title><link href="https://deveshparagiri.com/blog/2025/drift-configuration-eval-copy/" rel="alternate" type="text/html" title="Drift Configuration Evaluation and Transition Matrix Optimization"/><published>2025-06-14T16:00:00+00:00</published><updated>2025-06-14T16:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/drift-configuration-eval%20copy</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/drift-configuration-eval-copy/"><![CDATA[<h2 id="overview"><strong>Overview</strong></h2> <p>Building on our previous work analyzing spatial drift in NAIP imagery, we now shift focus to discovering <strong>globally optimal drift configurations</strong>. Specifically, we aim to:</p> <ul> <li>Evaluate all <strong>625 possible drift configurations</strong> (0–4 pixel shifts in row and column) between two years.</li> <li>Score each configuration using biologically inspired heuristics.</li> <li>Visualize and compare <strong>transition matrices</strong> from fixed vs. drift-corrected rasters.</li> <li>Statistically validate improvements in <strong>NDVI temporal consistency</strong> and <strong>transition plausibility</strong>.</li> </ul> <p>This report also introduces two scoring mechanisms to distill each 3×3 transition matrix into a single numeric value:</p> <ul> <li><strong>Simple Score</strong>: A linear weighting that rewards identity transitions and penalizes implausible ones.</li> <li><strong>Composite Score</strong>: A multi-factored function accounting for ecological decay patterns, entropy (stability), and biologically implausible reversals.</li> </ul> <hr/> <p><br/></p> <h2 id="why-625-drift-configurations"><strong>Why 625 Drift Configurations?</strong></h2> <p>NAIP imagery has a <strong>maximum documented spatial error of ±4 meters</strong>, or <strong>4 pixels (1m resolution)</strong> in either direction. For each pixel in <code class="language-plaintext highlighter-rouge">year 1</code>, we test all combinations of potential displacements for both the reference (<code class="language-plaintext highlighter-rouge">year 1</code>) and the target (<code class="language-plaintext highlighter-rouge">year 2</code>) images:</p> <blockquote> <p>Total drift combinations = 5 (row1) × 5 (col1) × 5 (row2) × 5 (col2) = 625</p> </blockquote> <p>For each configuration, we:</p> <ul> <li>Shift the raster accordingly.</li> <li>Compute a full <strong>3×3 transition matrix</strong>.</li> <li>Apply the scoring functions described below.</li> </ul> <hr/> <p><br/></p> <h2 id="scoring-transition-matrices"><strong>Scoring Transition Matrices</strong></h2> <p><br/></p> <h3 id="simple-score"><strong>Simple Score</strong></h3> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SCORE = +1 × identity transitions (LIVE→LIVE, DEAD→DEAD, BARE→BARE)
       - 3 × implausible transitions (e.g., BARE→LIVE, DEAD→LIVE, BARE→DEAD)
       - 2 × LIVE→BARE, DEAD→BARE
</code></pre></div></div> <p>This score linearly rewards stability and penalizes biologically suspect changes. Fast to compute, it provides an interpretable surface over the 625 space.</p> <blockquote> <p>We visualized this in 2D heatmaps (fixed dr1, dc1) and a 3D surface showing plausibility as a function of relative drift ∆r, ∆c.</p> </blockquote> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/reldrift2d.png" sizes="95vw"/> <img src="/assets/img/drift_config/reldrift2d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – Heatmap of Plausibility (Simple Score) Across Fixed Drift Combinations</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/reldrift3d.png" sizes="95vw"/> <img src="/assets/img/drift_config/reldrift3d.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – 3D Surface Plot of Drift ∆r, ∆c vs Score</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="composite-score"><strong>Composite Score</strong></h3> <p>The composite score integrates:</p> <ul> <li><strong>Decay Reward</strong>: Encourages LIVE → DEAD → BARE progression</li> <li><strong>Implausibility Penalty</strong>: Penalizes unnatural reversals (e.g., BARE → DEAD)</li> <li><strong>Entropy Penalty</strong>: Penalizes instability in row-wise transition distributions</li> </ul> <div class="language-text highlighter-rouge"><div class="highlight"><pre class="highlight"><code>SCORE = -Entropy + Decay Reward - Implausible Penalty
</code></pre></div></div> <p>This metric encodes more biological realism and rewards transitions that align with ecological degradation.</p> <p>We plotted this across all 625 configurations and observed a <strong>long-tailed unimodal curve</strong>:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/composite_score_plot.png" sizes="95vw"/> <img src="/assets/img/drift_config/composite_score_plot.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – Composite Score Distribution Over Drift Space</figcaption> </figure> </div> </div> <hr/> <h2 id="comparing-best-drift-configs"><strong>Comparing Best Drift Configs</strong></h2> <p>We selected the top-scoring drift configurations under both schemes:</p> <ul> <li><strong>Simple Score</strong>: <code class="language-plaintext highlighter-rouge">(0, 3, 0, 4)</code></li> <li><strong>Composite Score</strong>: <code class="language-plaintext highlighter-rouge">(1, 3, 1, 1)</code></li> </ul> <p>Below are the resulting transition matrices compared to the baseline (no drift):</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/simple_compare.png" sizes="95vw"/> <img src="/assets/img/drift_config/simple_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 4 – Simple Drift vs No Drift (Transition Matrix %)</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/composite_compare.png" sizes="95vw"/> <img src="/assets/img/drift_config/composite_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 5 – Composite Drift vs No Drift (Transition Matrix %)</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/simple_composite_compare.png" sizes="95vw"/> <img src="/assets/img/drift_config/simple_composite_compare.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 6 – Simple vs Composite Drift (Transition Matrix %)</figcaption> </figure> </div> </div> <p><br/></p> <h4 id="observation"><strong>Observation</strong></h4> <ul> <li>Simple drift yielded cleaner BARE → BARE diagonals and minimized reversal transitions</li> <li>Composite score slightly favored plausible but diverse transitions with less focus on strict diagonal preservation</li> </ul> <hr/> <p><br/></p> <h4 id="note-on-edge-effects-and-pixel-exclusion"><strong>Note on Edge Effects and Pixel Exclusion</strong></h4> <ul> <li> <p>When applying spatial drift configurations (e.g., shifting pixels up to 4 rows/columns in each direction), some border regions in the raster fall outside the valid image bounds. To ensure consistency and prevent out-of-bounds indexing errors, these edge pixels were excluded from all transition and NDVI calculations.</p> </li> <li> <p>As a result, the total number of pixels used in each drift configuration may vary slightly depending on how much drift was applied, especially in diagonally extreme configurations. These excluded pixels do not affect the comparative plausibility scoring, as the variation is negligible (&lt;3% of total pixels), and only valid overlapping regions were considered in all evaluations.</p> </li> </ul> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/count_distribution.png" sizes="95vw"/> <img src="/assets/img/drift_config/count_distribution.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="validating-ndvi-temporal-stability"><strong>Validating NDVI Temporal Stability</strong></h2> <p>To evaluate temporal alignment, we computed <strong>pixelwise NDVI standard deviation</strong> across 5 years (2014–2022) for all interior pixels:</p> <ul> <li><strong>Fixed Coordinates</strong>: Use same pixel (r, c) each year</li> <li><strong>Drift-Corrected</strong>: Apply optimal drift (from 2014 and 2016 only) then use fixed (r, c) for future years</li> </ul> <p><strong>Result:</strong></p> <table> <thead> <tr> <th>Metric</th> <th>Fixed</th> <th>Drift-Corrected</th> </tr> </thead> <tbody> <tr> <td>Mean NDVI Std Deviation</td> <td>0.04544</td> <td>0.05093</td> </tr> <tr> <td>Paired t-test p-value</td> <td><strong>&lt; 0.000001</strong></td> <td>(statistically significant)</td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/drift_config/ndvi.png" sizes="95vw"/> <img src="/assets/img/drift_config/ndvi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> </figure> </div> </div> <p>While the mean NDVI variability increased slightly under drift correction, the test remains a useful lens for validating long-term alignment.</p> <hr/> <p><br/></p> <h2 id="observing-score-differences"><strong>Observing Score Differences</strong></h2> <p>We analyzed the <strong>Spearman Rank Correlation</strong> between both scoring systems:</p> <blockquote> <p>ρ = -0.2253, p &lt; 0.000001 → Significant negative correlation</p> </blockquote> <p>This suggests the scoring systems are <strong>not directly aligned</strong>, and each captures different facets of temporal plausibility</p> <hr/> <p><br/></p> <h2 id="final-takeaways"><strong>Final Takeaways</strong></h2> <ul> <li>Optimal drift varies depending on scoring design and ecological intent</li> <li>Simple scoring performs surprisingly well for suppressing implausible transitions</li> <li>Composite scoring emphasizes ecological progression and entropy</li> <li>NDVI-based variance is slightly worse under drift correction, but this may reflect better alignment with shifting canopies or shadows</li> <li>This pipeline generalizes across time ranges and can be applied to all pairwise year transitions</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Apply the same analysis for 2016 → 2018, 2018 → 2020, 2020 → 2022</li> <li>Test alternate scoring functions (e.g., chi-squared divergence from expected decay)</li> <li>Integrate the best drift config into training/validation for improved pixel classification</li> <li>Track class-specific drift stability (e.g., BARE pixels across years)</li> <li>Eventually aim for full spatiotemporal drift-corrected land cover analysis</li> </ul> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Analyzing Spatial Drift in Aerial Imagery - Implications for Temporal Pixel Classification</title><link href="https://deveshparagiri.com/blog/2025/analyzing-spatial-drift/" rel="alternate" type="text/html" title="Analyzing Spatial Drift in Aerial Imagery - Implications for Temporal Pixel Classification"/><published>2025-06-04T16:00:00+00:00</published><updated>2025-06-04T16:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/analyzing-spatial-drift</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/analyzing-spatial-drift/"><![CDATA[<h2 id="motivation"><strong>Motivation</strong></h2> <p>Accurate analysis of temporal vegetation transitions—such as LIVE → DEAD or DEAD → BARE—requires reliable alignment of pixels across years. Even after spatial preprocessing like warping and histogram matching, small-scale spatial drift can occur, especially in high-resolution NAIP imagery.</p> <p>This poses a significant problem for longitudinal classification. If the same (row, col) location in one year corresponds to a slightly shifted feature in another (e.g., shadow, soil), observed changes may reflect misalignment rather than ecological dynamics.</p> <p>To rigorously evaluate class transitions and develop robust temporal models, we must quantify and control for this drift.</p> <hr/> <p><br/></p> <h2 id="identifying-a-fixed-reference-point"><strong>Identifying a Fixed Reference Point</strong></h2> <p>To ground the analysis, we manually located a visually bright, highly consistent white patch—most likely a man-made structure or tower—within the 2014 NAIP image. This was done using RGB previews of the matched<em>buffer</em>{year}.tif files in QGIS.</p> <p>We then traced this patch across earlier years and visually validated its shifted position in 2016 and 2018. The corresponding row/column locations are:</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>Row</strong></th> <th><strong>Col</strong></th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>119</td> <td>224</td> </tr> <tr> <td>2016</td> <td>121</td> <td>224</td> </tr> <tr> <td>2018</td> <td>118</td> <td>224</td> </tr> </tbody> </table> <p><br/> No precise match could be confirmed for 2020 and 2022, likely due to changes in lighting conditions or NDVI spectral compression in post-processing.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/fixedpixels.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/fixedpixels.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 1 – 2x2 Patch Across Years (Visual RGB Validation)</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="understanding-spatial-drift"><strong>Understanding Spatial Drift</strong></h2> <p>We define three conceptual degrees of spatial drift:</p> <ul> <li> <h6 id="level-1-linear-shift"><strong>Level 1: Linear Shift</strong></h6> <p>Straightforward pixel-level movement (±1–3 pixels), often due to image resampling or slight registration error.</p> </li> <li> <h6 id="level-2-rotationalangular-misalignment"><strong>Level 2: Rotational/Angular Misalignment</strong></h6> <p>Small-angle shifts or skewing that change the neighborhood context of a patch (i.e., rotated trees or canopy boundaries).</p> </li> <li> <h6 id="level-3-raster-wide-nonlinear-drift"><strong>Level 3: Raster-Wide Nonlinear Drift</strong></h6> <p>Region-specific distortions or warping effects that cannot be corrected via uniform translation.</p> </li> </ul> <p>In this study, we focused on evaluating Level 1 drift at the patch (5×5) and pixel levels.</p> <hr/> <h2 id="experimental-design"><strong>Experimental Design</strong></h2> <p>Given the difficulty of locating more visually stable points, we designed a randomized experiment to test whether NDVI time-series stability improves when correcting for drift.</p> <h3 id="key-questions"><strong>Key Questions</strong></h3> <ul> <li>Does accounting for local drift improve NDVI consistency over time?</li> <li>How does this effect vary between single pixels and aggregated 5×5 patches?</li> </ul> <h3 id="method"><strong>Method</strong></h3> <p>We sampled <strong>50 locations</strong> from the buffer region:</p> <ul> <li><strong>3 manually verified drifted tower locations</strong></li> <li><strong>47 random locations</strong> from valid areas of the 2014 image</li> </ul> <p>For each location, we:</p> <ul> <li>Extracted NDVI time-series from the same fixed coordinate over five years</li> <li>Applied drift correction by finding the best-matching 5×5 patch (or pixel) using RGB MSE</li> <li>Calculated standard deviation of NDVI across time in both cases</li> <li>Performed a paired t-test comparing NDVI temporal variability (std deviation) before and after correction</li> </ul> <hr/> <p><br/></p> <h2 id="results"><strong>Results</strong></h2> <p><br/></p> <h3 id="patch-level-55"><strong>Patch-Level (5×5)</strong></h3> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Fixed</strong></th> <th><strong>Corrected</strong></th> </tr> </thead> <tbody> <tr> <td>Mean NDVI std</td> <td>0.01902</td> <td>0.02078</td> </tr> <tr> <td>T-statistic</td> <td>-1.5629</td> <td> </td> </tr> <tr> <td>P-value</td> <td>0.12452</td> <td> </td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/patchdrift.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/patchdrift.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 2 – Histogram: NDVI Std Dev (5×5 Patch-Level, Fixed vs Corrected)</figcaption> </figure> </div> </div> <p><strong>Interpretation:</strong></p> <p>NDVI temporal variance was slightly higher after drift correction, but the difference was not statistically significant. This suggests that 5×5 patches may already average out small shifts, providing inherent spatial robustness.</p> <hr/> <p><br/></p> <h3 id="pixel-level-11"><strong>Pixel-Level (1×1)</strong></h3> <table> <thead> <tr> <th><strong>Metric</strong></th> <th><strong>Fixed</strong></th> <th><strong>Corrected</strong></th> </tr> </thead> <tbody> <tr> <td>Mean NDVI std</td> <td>0.03982</td> <td>0.02163</td> </tr> <tr> <td>T-statistic</td> <td>5.4654</td> <td> </td> </tr> <tr> <td>P-value</td> <td>0.000002</td> <td> </td> </tr> </tbody> </table> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/pixeldrift.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/pixeldrift.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Figure 3 – Histogram: NDVI Std Dev (Pixel-Level, Fixed vs Corrected)</figcaption> </figure> </div> </div> <p><strong>Interpretation:</strong></p> <p>At the pixel level, drift correction substantially reduced NDVI variance over time. This indicates that raw pixel comparisons are highly sensitive to even minor misalignments, validating the need for patch-based or drift-corrected strategies in pixelwise classification.</p> <hr/> <p><br/></p> <h2 id="implications-for-temporal-classification"><strong>Implications for Temporal Classification</strong></h2> <p>This experiment confirms that spatial drift—though subtle—can meaningfully distort pixel-level change analysis. Without drift correction:</p> <ul> <li>Apparent transitions may be artifacts</li> <li>NDVI profiles become unstable</li> <li>Model errors can accumulate over time</li> </ul> <p>In contrast, aggregating over 5×5 patches appears to mitigate these issues. For temporal studies involving land cover change, forest degradation, or regrowth detection, we recommend either:</p> <ol> <li>Drift correction per-pixel when doing fine-grained transition modeling, or</li> <li>Switching to patch-based classification frameworks.</li> </ol> <hr/> <p><br/></p> <h2 id="limitations-and-next-steps"><strong>Limitations and Next Steps</strong></h2> <ul> <li>Only 3 fixed reference points could be visually confirmed across years</li> <li>Drift was only modeled as linear (search radius ±3 pixels)</li> <li>NDVI was the only signal used for comparison (future versions may include full spectral MSE)</li> </ul> <hr/> <p><br/></p> <h2 id="class-transition-validation-under-drift-correction"><strong>Class Transition Validation Under Drift Correction</strong></h2> <p>To further quantify the impact of spatial drift, we evaluated how <strong>class transitions across years</strong> are affected by fixed vs. drift-corrected sampling. The goal was to check whether implausible transitions—e.g., BARE → DEAD or DEAD → LIVE—are more common when drift is not accounted for.</p> <hr/> <p><br/></p> <h3 id="motivation-1"><strong>Motivation</strong></h3> <p>Temporal classification relies not just on stable NDVI signals but also on <strong>reasonable class transitions</strong>. Certain transitions are biologically plausible (e.g., LIVE → DEAD → BARE), while others are not expected without long timescales or special conditions (e.g., BARE → LIVE over 2 years).</p> <p>Spatial drift can artificially introduce these implausible sequences. This section evaluates whether correcting for drift reduces the frequency of such transitions across the entire buffer.</p> <hr/> <p><br/></p> <h3 id="hypothesis"><strong>Hypothesis</strong></h3> <p><strong>If drift correction improves temporal alignment, then:</strong></p> <ul> <li>The proportion of implausible transitions should decrease</li> <li>Heatmaps of transition probabilities should show more stability or logical progression</li> <li>This effect should be statistically significant</li> </ul> <hr/> <p><br/></p> <h3 id="method-1"><strong>Method</strong></h3> <p>For each pixel in the valid interior region of the buffer:</p> <ol> <li><strong>Extract Class Predictions</strong> from 2014, 2016, and 2018</li> <li><strong>Construct Fixed Sequences</strong>: row/col is held constant</li> <li><strong>Construct Drift-Corrected Sequences</strong>: find the closest RGB patch match in the next year using 5×5 RGB context</li> <li><strong>Count transitions</strong> of form: (class_t → class_t+1) for each consecutive year pair</li> <li><strong>Tabulate</strong> transition matrices for both fixed and corrected cases</li> <li><strong>Highlight implausible transitions</strong>, defined as: <ul> <li>DEAD → LIVE</li> <li>BARE → DEAD</li> <li>BARE → LIVE</li> </ul> </li> <li><strong>Plot</strong> <ul> <li>Heatmaps of transition probabilities</li> <li>Bar plots of total implausible transition percentages</li> </ul> </li> </ol> <hr/> <p><br/></p> <h3 id="results-1"><strong>Results</strong></h3> <p><br/></p> <h3 id="heatmap-2014--2016-transitions"><strong>Heatmap: 2014 → 2016 Transitions</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/transition1.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/transition1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Heatmap: 2014 → 2016 Transition</figcaption> </figure> </div> </div> <h3 id="heatmap-2016--2018-transitions"><strong>Heatmap: 2016 → 2018 Transitions</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/transition2.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/transition2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Heatmap: 2016 → 2018 Transition</figcaption> </figure> </div> </div> <hr/> <h3 id="bar-chart-implausible-transition-rates"><strong>Bar Chart: Implausible Transition Rates</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/spatial_drift/graph1.png" sizes="95vw"/> <img src="/assets/img/spatial_drift/graph1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Bar Chart: Implausible Transition Rates</figcaption> </figure> </div> </div> <table> <thead> <tr> <th><strong>Transition Period</strong></th> <th><strong>Fixed (%)</strong></th> <th><strong>Corrected (%)</strong></th> </tr> </thead> <tbody> <tr> <td>2014 → 2016</td> <td>15.5%</td> <td>13.4%</td> </tr> <tr> <td>2016 → 2018</td> <td>19.8%</td> <td>14.4%</td> </tr> </tbody> </table> <p><br/> <strong>Statistical Test (Chi²):</strong></p> <ul> <li><strong>χ² Statistic:</strong> 144.07</li> <li><strong>P-value:</strong> &lt; 0.000001</li> </ul> <p>→ <strong>Statistically significant improvement</strong> in plausibility after drift correction</p> <hr/> <p><br/></p> <h3 id="interpretation"><strong>Interpretation</strong></h3> <ul> <li>Heatmaps show that <strong>drift correction increases class stability</strong>, particularly for BARE and DEAD categories.</li> <li>The <strong>frequency of implausible transitions drops significantly</strong> when drift is corrected.</li> <li>Statistical testing confirms that this reduction is unlikely to be due to chance.</li> </ul> <hr/> <p><br/></p> <h2 id="final-takeaways"><strong>Final Takeaways</strong></h2> <ul> <li>Pixel-level NDVI analysis is highly sensitive to spatial drift</li> <li>Drift correction reduces implausible transitions and enhances temporal stability</li> <li>Patch-level aggregation is somewhat resilient to drift but less interpretable</li> <li>This two-pronged analysis—NDVI time-series + transition plausibility—offers a comprehensive method for diagnosing and mitigating drift in remote sensing studies</li> </ul> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Motivation]]></summary></entry><entry><title type="html">Refining Tree Pixel Classification and Temporal Analysis with Updated Labels and Balancing</title><link href="https://deveshparagiri.com/blog/2025/refining-tree-pixel-classification.md/" rel="alternate" type="text/html" title="Refining Tree Pixel Classification and Temporal Analysis with Updated Labels and Balancing"/><published>2025-05-28T16:00:00+00:00</published><updated>2025-05-28T16:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/refining-tree-pixel-classification.md</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/refining-tree-pixel-classification.md/"><![CDATA[<h2 id="overview"><strong>Overview</strong></h2> <p>This report extends my previous work on classifying LIVE, DEAD, and BARE tree pixels in aerial imagery. It focuses on refining model performance through targeted labeling improvements, class rebalancing, and updated predictions across five years of NAIP imagery.</p> <p><br/></p> <h3 id="objectives"><strong>Objectives</strong></h3> <ul> <li>Improve class performance (especially DEAD) via more diverse and confident labeling</li> <li>Balance class counts to prevent overfitting to LIVE</li> <li>Apply the updated model to 2014–2022 rasters and evaluate class distribution trends</li> <li>Quantitatively validate whether observed changes in class proportions over time are statistically significant</li> <li>Explore visual and quantitative summaries of tree pixel transitions</li> </ul> <hr/> <p><br/></p> <h2 id="label-improvements-and-class-balance"><strong>Label Improvements and Class Balance</strong></h2> <p>A significant portion of this work involved adding high-confidence LIVE samples in underrepresented years (especially 2018 and 2022) and increasing the BARE count.</p> <p><br/></p> <h3 id="final-label-distribution"><strong>Final Label Distribution</strong></h3> <p><br/></p> <table> <thead> <tr> <th><strong>Class</strong></th> <th><strong>Count</strong></th> </tr> </thead> <tbody> <tr> <td>LIVE</td> <td>153</td> </tr> <tr> <td>DEAD</td> <td>113</td> </tr> <tr> <td>BARE</td> <td>103</td> </tr> </tbody> </table> <hr/> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>LIVE</strong></th> <th><strong>DEAD</strong></th> <th><strong>BARE</strong></th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>28</td> <td>23</td> <td>19</td> </tr> <tr> <td>2016</td> <td>39</td> <td>18</td> <td>16</td> </tr> <tr> <td>2018</td> <td>24</td> <td>22</td> <td>31</td> </tr> <tr> <td>2020</td> <td>38</td> <td>24</td> <td>11</td> </tr> <tr> <td>2022</td> <td>24</td> <td>26</td> <td>26</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h2 id="model-evaluation-8020-split"><strong>Model Evaluation (80/20 Split)</strong></h2> <p>Using the updated labeled dataset of 369 labeled pixels across five years, I trained a Random Forest classifier using an 80/20 stratified split and <code class="language-plaintext highlighter-rouge">class_weight="balanced"</code> to account for class imbalance. The confusion matrix and metrics below reflect performance on the held-out 20% test set.</p> <p><br/></p> <h3 id="confusion-matrix"><strong>Confusion Matrix</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/confusionmatrix.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/confusionmatrix.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="classification-report"><strong>Classification Report</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>              precision    recall  f1-score
LIVE            0.95       0.67     0.78
DEAD            0.62       0.86     0.72
BARE            0.87       0.87     0.87
</code></pre></div></div> <p><br/></p> <h3 id="observations"><strong>Observations</strong></h3> <ul> <li>DEAD recall improved significantly due to class weighting and additional examples</li> <li>LIVE still dominates in precision, but shows some confusion with DEAD</li> <li>BARE remains stable and accurately learned</li> </ul> <hr/> <p><br/></p> <h2 id="raster-map-comparison-pre-vs-post"><strong>Raster Map Comparison (Pre vs Post)</strong></h2> <p>To assess model improvements in the wild, I compared raster predictions for 2020 before and after balancing:</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/2020premap.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/2020premap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">2020 Prediction Map (Pre-Balanced)</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/2020postmap.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/2020postmap.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">2020 Prediction Map (Post-Balanced)</figcaption> </figure> </div> </div> <p><br/></p> <table> <thead> <tr> <th><strong>Class</strong></th> <th><strong>Pre-Balanced</strong></th> <th><strong>Post-Balanced</strong></th> <th><strong>Change (%)</strong></th> </tr> </thead> <tbody> <tr> <td>LIVE</td> <td>21,628</td> <td>21,241</td> <td>-1.8%</td> </tr> <tr> <td>DEAD</td> <td>30,339</td> <td>31,112</td> <td>+2.5%</td> </tr> <tr> <td>BARE</td> <td>74,156</td> <td>73,770</td> <td>-0.5%</td> </tr> </tbody> </table> <p><br/></p> <h3 id="observation"><strong>Observation</strong></h3> <p>Balancing helped reduce DEAD under prediction and slightly corrected LIVE overconfidence.</p> <hr/> <p><br/></p> <h2 id="ndvi-histogram-by-class"><strong>NDVI Histogram (By Class)</strong></h2> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/ndvihistogram.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/ndvihistogram.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">NDVI Histogram (By Class)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="interpretation"><strong>Interpretation</strong></h3> <ul> <li>LIVE pixels form a clear NDVI peak between 0.22–0.30</li> <li>DEAD overlaps with both LIVE and BARE, consistent with its ambiguous spectral signature</li> <li>BARE clusters near NDVI &lt; 0.1</li> </ul> <hr/> <p><br/></p> <h2 id="predicted-maps-across-time-20142022"><strong>Predicted Maps Across Time (2014–2022)</strong></h2> <p>The updated model was applied to all years in the dataset.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/combined_visualization.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/combined_visualization.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Predicted Rasters (All Years)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="notable-observations"><strong>Notable Observations</strong></h3> <ul> <li>2014/2016: More DEAD patches, especially in the southern region</li> <li>2018/2020: Recovery in LIVE pixels in central/northern zones</li> <li>2022: Strong LIVE presence with balanced BARE–DEAD structure</li> </ul> <hr/> <p><br/></p> <h2 id="class-distribution-trends-pixel-level"><strong>Class Distribution Trends (Pixel-Level)</strong></h2> <p><br/></p> <h3 id="grouped-bar-chart"><strong>Grouped Bar Chart</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/stackedgraph.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/stackedgraph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Pixel Distribution Bar Chart (By Class)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="line-chart-per-class-over-time"><strong>Line Chart (Per Class Over Time)</strong></h3> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/refining_tree_pixel/linegraph.png" sizes="95vw"/> <img src="/assets/img/refining_tree_pixel/linegraph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Mortality Trend (2014-2022)</figcaption> </figure> </div> </div> <p><br/></p> <h3 id="insights"><strong>Insights</strong></h3> <ul> <li>BARE remains consistent (~45–47%) — acts as a stable control</li> <li>DEAD rises in 2016–2020, then drops in 2022</li> <li>LIVE shows a recovery in 2022 after dip in 2016–2018</li> </ul> <hr/> <p><br/></p> <h2 id="statistical-validation-chi-square-test"><strong>Statistical Validation: Chi-Square Test</strong></h2> <p>To test whether the distribution of class predictions changed meaningfully across years, I ran a chi-square test on predicted class counts.</p> <p><br/></p> <h3 id="results"><strong>Results</strong></h3> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Chi2 Statistic: 3688.71
Degrees of Freedom: 8
P-value: &lt; 0.000001
</code></pre></div></div> <p><br/></p> <h3 id="interpretation-1"><strong>Interpretation</strong></h3> <p>The test confirms that class distributions are <strong>not independent across years</strong> — vegetation structure has changed significantly over time.</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>LIVE (Obs)</strong></th> <th><strong>LIVE (Exp)</strong></th> <th><strong>DEAD (Obs)</strong></th> <th><strong>DEAD (Exp)</strong></th> <th><strong>BARE (Obs)</strong></th> <th><strong>BARE (Exp)</strong></th> </tr> </thead> <tbody> <tr> <td>2014</td> <td>14035</td> <td>17810.18</td> <td>41409</td> <td>35921.25</td> <td>70185</td> <td>71897.57</td> </tr> <tr> <td>2016</td> <td>17560</td> <td>17810.18</td> <td>38873</td> <td>35921.25</td> <td>69196</td> <td>71897.57</td> </tr> <tr> <td>2018</td> <td>18983</td> <td>17880.21</td> <td>34204</td> <td>36062.50</td> <td>72936</td> <td>72180.29</td> </tr> <tr> <td>2020</td> <td>21241</td> <td>17880.21</td> <td>31112</td> <td>36062.50</td> <td>73770</td> <td>72180.29</td> </tr> <tr> <td>2022</td> <td>17442</td> <td>17880.21</td> <td>34432</td> <td>36062.50</td> <td>74249</td> <td>72180.29</td> </tr> </tbody> </table> <hr/> <p><br/></p> <h2 id="conclusions"><strong>Conclusions</strong></h2> <ul> <li>Class balancing significantly improved DEAD prediction</li> <li>The model generalizes well across years, both visually and statistically</li> <li>NDVI remains the most valuable signal for distinguishing LIVE and BARE</li> <li>Predicted maps reflect meaningful ecological shifts — particularly post-2020 regrowth</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Incorporate 3×3 or 5×5 patch features to provide spatial context</li> <li>Analyze temporal transitions such as LIVE → DEAD or DEAD → BARE</li> <li>Investigate spatial drift between years by comparing prediction shifts in fixed coordinates</li> <li>Validate results with external field survey data or high-res canopy maps</li> <li>Expand the labeling dataset to include denser forest sections for generalization</li> </ul> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[Overview]]></summary></entry><entry><title type="html">Evaluating NDVI Based Tree Classification and Label Efficiency</title><link href="https://deveshparagiri.com/blog/2025/label-efficiency-and-generalization-copy/" rel="alternate" type="text/html" title="Evaluating NDVI Based Tree Classification and Label Efficiency"/><published>2025-05-26T20:00:00+00:00</published><updated>2025-05-26T20:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/label-efficiency-and-generalization%20copy</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/label-efficiency-and-generalization-copy/"><![CDATA[<hr/> <p><br/></p> <h2 id="overview"><strong>Overview</strong></h2> <p>This experiment investigates how accurately I can classify individual pixels in aerial imagery as <strong>LIVE</strong>, <strong>DEAD</strong>, or <strong>BARE</strong> ground using a small number of labeled examples. I evaluate how performance scales with label count, examine the benefit of including <strong>NDVI</strong> as an explicit input feature, and explore how well a model trained on <strong>multi-year data</strong> generalizes across time.</p> <p>I aim to answer the following core questions:</p> <ol> <li>How many labeled pixels per class are needed to achieve high accuracy?</li> <li>Does adding NDVI improve model stability and performance in low-data regimes?</li> <li>Can a model trained on a single year generalize to another year?</li> <li>How well does a model trained on labeled data from all years perform when evaluated across time?</li> </ol> <hr/> <p><br/></p> <h2 id="data-and-setup"><strong>Data and Setup</strong></h2> <ul> <li>All pixels were extracted from spectrally and spatially normalized NAIP imagery (2014–2022).</li> <li>Manual labels were created by visually inspecting .png previews across years for each sampled coordinate.</li> <li>Each labeled pixel was represented using either: <ul> <li><strong>4-band spectral input</strong>: Red, Green, Blue, NIR</li> <li><strong>5-band input</strong>: Red, Green, Blue, NIR, NDVI</li> </ul> </li> </ul> <p>The model used in all experiments is a RandomForestClassifier from sklearn.</p> <hr/> <p><br/></p> <h2 id="experiment-1-accuracy-vs-label-count-with-and-without-ndvi"><strong>Experiment 1: Accuracy vs Label Count (With and Without NDVI)</strong></h2> <p>I trained the classifier using only labeled pixels from <strong>2014</strong>. For each value of k (samples per class), I ran 5 randomized 80/20 train-test splits and recorded mean accuracy and standard deviation.</p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/accuracy_simple.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/accuracy_simple.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Accuracy vs Label Count (Without NDVI)</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/accuracy_ndvi.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/accuracy_ndvi.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Accuracy vs Label Count (With NDVI)</figcaption> </figure> </div> </div> <h3 id="results-summary"><strong>Results Summary</strong></h3> <table> <thead> <tr> <th><strong>Labeled/Class (k)</strong></th> <th><strong>Accuracy (w/o NDVI)</strong></th> <th><strong>Accuracy (w/ NDVI)</strong></th> </tr> </thead> <tbody> <tr> <td>5</td> <td>~52% ± high variance</td> <td>~74% ± lower variance</td> </tr> <tr> <td>8</td> <td>~65%</td> <td>~77%</td> </tr> <tr> <td>13</td> <td>~75%</td> <td><strong>~85%</strong></td> </tr> </tbody> </table> <p>In the first experiment, I found that adding NDVI to the input significantly improves model performance, especially at low sample counts. <br/></p> <h5 id="key-takeaways"><strong>Key Takeaways:</strong></h5> <ul> <li>NDVI significantly boosts accuracy, especially at <strong>low sample counts</strong></li> <li>Including NDVI stabilizes model performance across random splits</li> <li>Without NDVI, the model struggles to distinguish classes under limited supervision</li> <li>With NDVI, LIVE pixels are often learned with high confidence even with 5–6 examples</li> </ul> <hr/> <p><br/></p> <h2 id="experiment-2-cross-year-generalization-train-on-2014--test-on-2020"><strong>Experiment 2: Cross-Year Generalization (Train on 2014 → Test on 2020)</strong></h2> <p>I trained a model on all 2014-labeled pixels (using 5-band input with NDVI) and tested it directly on labeled pixels from 2020 — no retraining or adaptation.</p> <p><strong>Results:</strong></p> <ul> <li><strong>Overall Accuracy</strong>: <strong>78.4%</strong></li> <li><strong>LIVE</strong> class had high precision and recall</li> <li><strong>BARE</strong> was consistently predicted correctly (recall = 1.0), though with some over-prediction</li> <li><strong>DEAD</strong> remained harder to capture (recall = 0.53)</li> </ul> <div class="language-plaintext highlighter-rouge"><div class="highlight"><pre class="highlight"><code>Classification Report:
              precision    recall  f1-score
    BARE         0.571     1.000     0.727
    DEAD         0.818     0.529     0.643
    LIVE         0.885     0.885     0.885
</code></pre></div></div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/confusion_2014to2020.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/confusion_2014to2020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h2 id="implications"><strong>Implications</strong></h2> <ul> <li><strong>NDVI improves generalization</strong> by providing a vegetation-specific signal that remains valid across years.</li> <li>The model is most confident on <strong>LIVE</strong> pixels, suggesting that greenness + NDVI are strong predictors.</li> <li><strong>DEAD</strong> and <strong>BARE</strong> are more difficult to distinguish — these likely require: <ul> <li>More training samples</li> <li>Spatial or temporal context (e.g., adjacent pixels, change over time)</li> </ul> </li> </ul> <hr/> <p><br/></p> <h2 id="conclusions"><strong>Conclusions</strong></h2> <ul> <li>Based on this experiment, with as few as <strong>10–13 labeled pixels per class</strong>, we can reach <strong>&gt;85% accuracy</strong> using NDVI.</li> <li>Training on one year and applying to another is feasible if the data is normalized.</li> <li>NDVI should be included as a feature — it boosts performance significantly and reduces label burden.</li> </ul> <hr/> <p><br/></p> <h2 id="next-steps"><strong>Next Steps</strong></h2> <ul> <li>Increase label count across years</li> <li>Test reverse generalization: 2020 → 2014</li> <li>Predict full raster maps using trained models</li> <li>Potentially Add 3×3 or 5×5 patch-based context around each pixel</li> </ul> <hr/> <p><br/></p> <h2 id="experiment-3-combined-year-generalization-and-evaluation"><strong>Experiment 3: Combined-Year Generalization and Evaluation</strong></h2> <p>In this experiment, I explored how well a model trained on <strong>labeled pixels from all years combined</strong> performs across time. Rather than training on a single year, I sampled <code class="language-plaintext highlighter-rouge">k = 10–25</code> pixels per class (LIVE, DEAD, BARE) from the full labeled dataset and tested the model in two complementary ways:</p> <ol> <li><strong>Per-Year Generalization:</strong> Test accuracy is measured individually on each year (2014–2022).</li> <li><strong>80/20 Mixed-Year Accuracy:</strong> A stratified 80/20 split is used across all labeled data, simulating a more randomized evaluation.</li> </ol> <hr/> <p><br/></p> <h3 id="label-distribution"><strong>Label Distribution</strong></h3> <p>Before running this experiment, I reviewed how labeled samples were distributed across years and classes:</p> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>LIVE</strong></th> <th><strong>DEAD</strong></th> <th><strong>BARE</strong></th> <th><strong>Label Count Range (max - min)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>2014</strong></td> <td>19</td> <td>19</td> <td>13</td> <td>6</td> </tr> <tr> <td><strong>2016</strong></td> <td>30</td> <td>13</td> <td>8</td> <td>22</td> </tr> <tr> <td><strong>2018</strong></td> <td>15</td> <td>12</td> <td>24</td> <td>12</td> </tr> <tr> <td><strong>2020</strong></td> <td>26</td> <td>17</td> <td>8</td> <td>18</td> </tr> <tr> <td><strong>2022</strong></td> <td>16</td> <td>16</td> <td>19</td> <td>3</td> </tr> <tr> <td><strong>Total</strong></td> <td><strong>106</strong></td> <td><strong>77</strong></td> <td><strong>72</strong></td> <td> </td> </tr> </tbody> </table> <hr/> <p>While the class imbalance isn’t extreme, some years (like 2016 and 2020) had disproportionately more LIVE samples than BARE or DEAD. This may partially explain performance variation across years.</p> <hr/> <p><br/></p> <h3 id="graph-1-accuracy-per-year-trained-on-all-years"><strong>Graph 1: Accuracy per Year (Trained on All Years)</strong></h3> <p>I trained the model on all years using k samples per class and evaluated it year-by-year to assess generalization over time. <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/generalized_all.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/generalized_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Generalized Model applied to each year</figcaption> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Accuracy improves with label count but flattens after ~20 samples/class.</li> <li><strong>2022 consistently outperformed other years</strong>, reaching over 90% accuracy.</li> <li>2014 and 2016 showed slightly lower accuracy, likely due to noisier labels or less distinctive spectral features.</li> </ul> <hr/> <p><br/></p> <h3 id="graph-2-accuracy-vs-k-8020-random-split"><strong>Graph 2: Accuracy vs k (80/20 Random Split)</strong></h3> <p>For comparison, I also performed a standard 80/20 train-test split on the full dataset. <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/generalized_8020.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/generalized_8020.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Generalized Model Accuracy vs k</figcaption> </figure> </div> </div> <p><strong>Observations:</strong></p> <ul> <li>Accuracy was more variable at lower k values due to randomness in class composition.</li> <li>With 20+ samples per class, performance stabilized and closely matched the per-year evaluation curve.</li> </ul> <hr/> <p><br/></p> <h3 id="confusion-matrices-per-year"><strong>Confusion Matrices per Year</strong></h3> <p>To better understand how each class was predicted over time, I generated confusion matrices for each year using the model trained on all data (k=25 per class). <br/></p> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/label_efficiency_and_generalization/confusion_all.png" sizes="95vw"/> <img src="/assets/img/label_efficiency_and_generalization/confusion_all.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Confusion Matrix for Generalized Model Applied to Each Year</figcaption> </figure> </div> </div> <hr/> <p><br/></p> <h3 id="per-year-breakdown"><strong>Per-Year Breakdown</strong></h3> <table> <thead> <tr> <th><strong>Year</strong></th> <th><strong>Accuracy</strong></th> <th><strong>F1 (LIVE)</strong></th> <th><strong>F1 (DEAD)</strong></th> <th><strong>F1 (BARE)</strong></th> </tr> </thead> <tbody> <tr> <td><strong>2014</strong></td> <td>0.837</td> <td>0.778</td> <td>0.878</td> <td>0.741</td> </tr> <tr> <td><strong>2016</strong></td> <td>0.867</td> <td>0.778</td> <td>0.923</td> <td>0.667</td> </tr> <tr> <td><strong>2018</strong></td> <td>0.898</td> <td>0.786</td> <td>0.846</td> <td>0.957</td> </tr> <tr> <td><strong>2020</strong></td> <td>0.875</td> <td>0.846</td> <td>0.903</td> <td>0.778</td> </tr> <tr> <td><strong>2022</strong></td> <td><strong>0.901</strong></td> <td>0.786</td> <td>0.824</td> <td><strong>1.000</strong></td> </tr> </tbody> </table> <hr/> <p><br/></p> <h3 id="analysis-and-hypotheses"><strong>Analysis and Hypotheses</strong></h3> <ul> <li><strong>LIVE pixels were consistently learned well</strong> across all years, with F1 scores between 0.77–0.85.</li> <li><strong>BARE improved sharply in later years</strong>, especially in 2022 where it reached perfect precision and recall.</li> <li><strong>DEAD remained the most ambiguous</strong>, frequently confused with both LIVE and BARE. Its spectral profile is more variable and likely requires additional temporal or spatial information.</li> <li><strong>2022 performed best</strong>, possibly due to: <ul> <li>Balanced label distribution across all classes (All three classes are within a 3-count range)</li> <li>Better image quality or spectral separation</li> <li>Fewer mislabels from manual annotation</li> </ul> </li> </ul> <blockquote> <p class="block-tip">The generalized model’s consistent performance across years confirms that the <strong>spatial and spectral normalization approach</strong> was effective.</p> </blockquote> <p><br/></p> <h3 id="how-many-labels-are-enough"><strong>How Many Labels Are Enough?</strong></h3> <p>From all experiments, I observed that performance gains are <strong>nonlinear</strong> with respect to label count (logarithmic):</p> <ul> <li>The steepest gains occur between 5 and 15 samples/class.</li> <li>Accuracy improvements <strong>flatten beyond ~20 samples/class</strong>, suggesting diminishing returns.</li> <li>For robust, multi-year generalization: <ul> <li><strong>20–25 samples/class</strong> per year is ideal</li> <li>Alternatively, ~100 well-distributed samples/class across years can generalize effectively</li> </ul> </li> </ul> <hr/> <p><br/></p> <h3 id="takeaways"><strong>Takeaways</strong></h3> <ul> <li>Combined-year training leads to a <strong>stable, high-performing model</strong> across time.</li> <li><strong>NDVI continues to be critical</strong>, especially for identifying LIVE vegetation.</li> <li><strong>Confusion patterns reveal that DEAD remains the weakest class</strong>, needing more contextual signals.</li> <li>Training on all available data provides stronger generalization than per-year splits or single-year baselines.</li> </ul> <hr/> <p><br/></p> <h3 id="next-steps-1"><strong>Next Steps</strong></h3> <ul> <li>Increase the number of labeled pixels per class to 30–40 to further reduce variance.</li> <li>Investigate spatial context by incorporating patch-level features (e.g., 3×3 or 5×5 neighborhoods).</li> </ul> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry><entry><title type="html">Exploring Tree Mortality Detection Approaches from Aerial Imagery</title><link href="https://deveshparagiri.com/blog/2025/research-logs/" rel="alternate" type="text/html" title="Exploring Tree Mortality Detection Approaches from Aerial Imagery"/><published>2025-05-18T20:00:00+00:00</published><updated>2025-05-18T20:00:00+00:00</updated><id>https://deveshparagiri.com/blog/2025/research-logs</id><content type="html" xml:base="https://deveshparagiri.com/blog/2025/research-logs/"><![CDATA[<div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/maingraph.png" sizes="95vw"/> <img src="/assets/img/research_log/maingraph.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption"></figcaption> </figure> </div> </div> <p>The sharp divergence between ED-Lidar reconstructions, Landsat NDVI, and AmeriFlux GPP observations (see figure) around 2010–2020 sparked this investigation. While historical reconstructions suggest stable or even rising productivity, both satellite vegetation indices (NDVI) and flux tower data show a clear decline in GPP at the US-MPJ site during this period.</p> <p>This discrepancy raised a key question: <em>Is this ecosystem experiencing large-scale tree mortality that is not being captured by traditional models?</em></p> <p>To answer this, I developed an image-based approach to directly detect vegetation loss and tree death from aerial imagery, aiming to complement and explain these broader ecosystem signals. This report is a log of various initial approaches which were not pursued further due to inconclusive results. The final log throws light on the current approach being pursued due to its promising nature. The later research reports help to track tree mortality and its various real-world applications for NASA CMS etc.</p> <hr/> <h3 id="deepforest-approach"><strong>DeepForest Approach</strong></h3> <hr/> <p>I focus on how to reliably detect <strong>tree mortality</strong> across time using <strong>aerial imagery</strong>? Can <strong>pretrained object detection models</strong>, like <strong>DeepForest</strong>, give us useful indicators of <strong>tree health or death</strong>?</p> <p><a href="https://github.com/weecology/DeepForest">DeepForest</a> is a SOTA model trained on RGB imagery to detect <strong>individual tree crowns</strong>. It outputs <strong>bounding boxes</strong> around tree-like objects, which initially seemed promising for analyzing:</p> <ul> <li><strong>Tree count changes over time</strong></li> <li><strong>Tree canopy shrinkage</strong></li> <li>Mortality via <strong>absence or degradation</strong> of detected crowns</li> </ul> <hr/> <p>I use a pretrained model to quickly extract structured detections and use <strong>bounding box count or size</strong> as a proxy for forest density. This would be the first step towards detecting <strong>tree loss trends over time</strong> without training from scratch. Iused NAIP imagery from 2014-2022 split into <code class="language-plaintext highlighter-rouge">512x512</code> tiles. One interesting thing to note was the resolution change from 1.0m to 0.6m during the transition year of 2018.</p> <hr/> <div class="language-python highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="kn">from</span> <span class="n">deepforest</span> <span class="kn">import</span> <span class="n">main</span>
<span class="n">model</span> <span class="o">=</span> <span class="n">main</span><span class="p">.</span><span class="nf">deepforest</span><span class="p">()</span>
<span class="n">model</span><span class="p">.</span><span class="nf">use_release</span><span class="p">()</span>
<span class="n">boxes</span> <span class="o">=</span> <span class="n">model</span><span class="p">.</span><span class="nf">predict_image</span><span class="p">(</span><span class="n">path</span><span class="o">=</span><span class="sh">"</span><span class="s">naip_tile.png</span><span class="sh">"</span><span class="p">,</span> <span class="n">return_plot</span><span class="o">=</span><span class="bp">True</span><span class="p">)</span>
</code></pre></div></div> <hr/> <h4 id="sample-outputs"><strong>Sample Outputs</strong></h4> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log1.png" sizes="95vw"/> <img src="/assets/img/research_log/log1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepForest detection results before parameter tuning</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log1-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log1-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">DeepForest detection results after parameter tuning</figcaption> </figure> </div> </div> <p>While promising, the spatial resolution mismatch meant Icould not perform any temporal analysis. To add, DeeForest was trained on ~0.1m resolution which led to noisy predictoins due to the coarse nature of our NAIP tiles (1.0m/0.6m). Another key issue was that bounding boxes around trees did not necessarily translate to tree area, invalidating any use of true detection – which varied erratically due to visual artifacts.</p> <p>In summary, visual interpretability, spatial precision, and temporal consistency were all poor leading to exploring alternative approaches.</p> <hr/> <h3 id="patch-based-classification"><strong>Patch-Based Classification</strong></h3> <hr/> <p>I needed a strategy focused on semantic labeling (LIVE / DEAD / BARE), not detection. This led to the <code class="language-plaintext highlighter-rouge">5x5</code> patch-based classification approach. The goal was to label small regions of aerial imagery based on <strong>ecological intuition</strong>, using spatial patches instead of pixel-level or bounding box classification.</p> <p>Each 512×512 tile was divided into a <code class="language-plaintext highlighter-rouge">5×5</code> grid (i.e. ~25×25 pixel patches). Each patch was visually labeled based on <strong>dominant appearance</strong>: LIVE, DEAD, or BARE. The thought process was this was smoother than pixel classification yet easier than labeling full tiles while providing enough context for human labeling (e.g., sparse vs dense canopy).</p> <p>I recorded each tile into patches with relevant metadata. A sample record would read as <code class="language-plaintext highlighter-rouge">r329_c58_y2020.png, 329, 58, 2020, , possibly BARE or DEAD</code>. Labels were then assigned via manual inspection across year with texture, color and shape being taken into account. I then make a simple RandomForest model (baseline) with RGB stats as our core features.</p> <hr/> <h4 id="sample-outputs-1"><strong>Sample Outputs</strong></h4> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-1.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Sample patch labeling</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Area of Interest</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-3.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-3.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Dead Tree Area Over Time</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-12 mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log2-4.png" sizes="95vw"/> <img src="/assets/img/research_log/log2-4.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Transition Matrices for 5x5 Patch Classification</figcaption> </figure> </div> </div> <hr/> <p>This was a promising direction but had drawbacks. For instance, I encountered labeling conflicts with mixed-content patches (e.g., half LIVE, half DEAD) making it very subjective. The spectral and spatial inconsistencies were more complex at the patch level with the model failing to generalize across years.</p> <hr/> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>In-year Accuracy</td> <td>~65–70%</td> </tr> <tr> <td>Cross-year Generalization</td> <td>Failed (&lt;50%)</td> </tr> <tr> <td>Label Noise</td> <td>High</td> </tr> </tbody> </table> <hr/> <h3 id="single-pixel-temporal-classification"><strong>Single Pixel Temporal Classification</strong></h3> <hr/> <p>To resolve both spectral and spatial instability, I transitioned to <strong>pixel-level temporal modeling</strong> — tracking <strong>individual pixels across all years</strong> and use their <strong>temporal NDVI trajectories</strong> to classify them into ecological categories (LIVE, DEAD, BARE). With this approach, I now gain control over exact spatial location and can detect transitions across years. I also included NDVI data as part of the labeling strategy.</p> <hr/> <h4 id="sample-outputs-2"><strong>Sample Outputs</strong></h4> <hr/> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/raw_buffer_2014.png" sizes="95vw"/> <img src="/assets/img/research_log/raw_buffer_2014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Raw Buffer 2014</figcaption> </figure> </div> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/matched_buffer_2014.png" sizes="95vw"/> <img src="/assets/img/research_log/matched_buffer_2014.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Spectrally Matched Buffer 2014</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log3-2.png" sizes="95vw"/> <img src="/assets/img/research_log/log3-2.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Vegetation Reflectance Drift</figcaption> </figure> </div> </div> <div class="row mt-3"> <div class="col-sm mt-3 mt-md-0"> <figure> <picture> <source class="responsive-img-srcset" srcset="/assets/img/research_log/log3-1.png" sizes="95vw"/> <img src="/assets/img/research_log/log3-1.png" class="img-fluid rounded z-depth-1" width="100%" height="auto" data-zoomable="" loading="eager" onerror="this.onerror=null; $('.responsive-img-srcset').remove();"/> </picture> <figcaption class="caption">Single Pixel Time Series</figcaption> </figure> </div> </div> <hr/> <p>The benefits of this approach were multifold. Now, I can solve for spectral inconsistencies and apply spatial registration corrections to study transitions with high accuracy.</p> <hr/> <table> <thead> <tr> <th>Metric</th> <th>Result</th> </tr> </thead> <tbody> <tr> <td>Temporal NDVI consistency</td> <td>Strong</td> </tr> <tr> <td>Human label quality</td> <td>High confidence</td> </tr> <tr> <td>Class balance</td> <td>Still tuning</td> </tr> <tr> <td>Next step</td> <td>Expand labeled samples &amp; train temporal classifier</td> </tr> </tbody> </table> <hr/> <p>Refer <a href="/blog/tag/research/">here</a> for all research reports.</p>]]></content><author><name></name></author><category term="code"/><category term="ai,"/><category term="research"/><summary type="html"><![CDATA[]]></summary></entry></feed>